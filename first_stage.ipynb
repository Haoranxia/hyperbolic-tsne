{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greenlight presentation notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current progress\n",
    "\n",
    "tldr: HyperbolicKL has some issues, we switch to GaussianKL to overcome them (hopefully)\n",
    "\n",
    "### HyperbolicKL notes\n",
    "I'm trying to figure out what exactly is going wrong when certain flags are set:\n",
    "1. Baseline experiment: (MNIST, wrong_grad, no scale_fix, HyperbolicKL, accel. on/off) \\\n",
    "   Works perfectly good. We get the results matching the paper (baseline)\n",
    "\n",
    "2. Experiment 2: (MNIST, wrong_grad, scale_fix, HyperbolicKL, acce. on/off) \\\n",
    "   Works fairly well. We get results similar to baseline. But it requires (much) more iterations to converge nicely.\n",
    "   This is due to the scaling of the inverse metric tensor term. Since it basically scales the gradients down consistently.\n",
    "\n",
    "3. Experiment 3: (MNIST, correct_gad, no scale_fix, HyperbolicKL, accel. on/off) \\\n",
    "   Takes much longer to converge properly. The extra d^H_ij term makes gradients (near) 0 for a long time.\n",
    "   Gradients get very large too at some point (embeddings overshoot the Disk)\n",
    "\n",
    "4. Experiment 4: (MNIST, correct_grad, scale_fix, HyperbolicKL, accel. on/off) \\\n",
    "   Takes even longer to converge properly. Now we have both d^H_ij (initially) and scale_fix scaling the gradients down. \\\n",
    "   However now gradients stay within reasonable amounts and don't explode off to large values. Presumably due to the scale_fix \\\n",
    "   These settings reproduce the result where points are pushed towards the boundary (conceptual sensible one)\n",
    "\n",
    "In points (3, 4) we get other issues. So the fact that gradients are very small means no progress is made for a long time, means the algorithm just stops.\n",
    "\\\n",
    "If we use both fixes (gradient, scale_fix) then we get the \"issue\" that points are pushed towards the boundary. \n",
    "\n",
    "To avoid early stopping increase **n_iter_check** flag in **opt_config** and manually change **n_iter_without_progress** in **solver.py**\n",
    "\n",
    "#### HyperbolicKL on Tree data\n",
    "Does not work very well. Often points don't even move from the center or other weirdness happens. I haven't tested this specific case much yet though.\n",
    "\n",
    "#### HyperbolicKL on Real data\n",
    "Pushes points towards the boundaries much more strongly than GaussianKL\n",
    "\\\n",
    "See experiment (73, 74)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperbolicKL to GaussianKL Motivation\n",
    "\n",
    "Gaussian has faster decreasing tails, resulting in forces being propegated out less, so embeddings are less likely to be pushed outwards towards the edge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianKL notes\n",
    "\n",
    "Some general notes:\n",
    "\n",
    "- size_tol parameter and the Hyperbolic variance really affect the embedding. This requires some hand-tuning\n",
    "- Early exaggeration \n",
    "- BH accel. numerical problems\n",
    "- Large tree depth problems\n",
    "- Crowding problem? Clusters being squished to points?\n",
    "- GaussianKL gradient: (2/var term and learning rate)\n",
    "\n",
    "#### GaussianKL on Tree dataset\n",
    "On the tree dataset, GaussianKL seems to produce much better results than HyperbolicKL. Although there are some issues.\n",
    "\n",
    "#### GaussianKL on Real datasets (MNIST, C_ELEGANS for now)\n",
    "On the real datasets, GaussianKL does not push embeddings towards the boundaries as much as HyperbolicKL. \n",
    "\\\n",
    "See experiment (71, 72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree dataset\n",
    "\n",
    "To properly test the notion that tree-like data can be nicely embedded in Hyperbolic Space, we turn to an artificially created tree-like dataset. \n",
    "\\\n",
    "I basically wrote some code that generates a Distance **D** and affinity **V** Matrix such that its entries reflect a tree-like ordering of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "Some notable experiments and analysis:\n",
    "\n",
    "## Laptop Experiments\n",
    "#### Experiment 38 (also 68)\n",
    "This experiment nicely showcases a tree embedding using GaussianKL. However, this experiment had an extra grad \\*= 4 term in the cost function (leftover when I coped over the HyperbolicKL code) that caused the embedding to converge to this specific visualization.\n",
    "\\\n",
    "\\\n",
    "If this term is removed, the clusters are more dense and converge to points. (experiment 66)\n",
    "\\\n",
    "\\\n",
    "However, we can regain clusters by adjusting the learning rate (this basically mimics the grad \\*=4 mistake).\n",
    "It seems that adjusting the learning rate will give us \"better\" (subjective and context-dependent) embeddings.\n",
    "\n",
    "\n",
    "#### Experiment 73. 74\n",
    "Showcases HyperbolicKL on MNIST. Points are pushed along the boundary very strongly\n",
    "\n",
    "#### Experiment 71, 72\n",
    "Showcases GaussianKL on MNIST. Points are much less pushed toward the center and the embedding actually takes shape inside the visible part of the disk.\n",
    "\n",
    "#### Experiment 83, 84\n",
    "Attempt to reproduce 38, 68 without the grad \\*= 4 term, but by adjusting the learning rate since it achieves basically the same result. This attempt was succesful. Therefore learning rate really affects the embeddings.\n",
    "\n",
    "#### Experiment 85, 86\n",
    "Experiment that showcases how adjusting learning rate also improves embeddings for larger trees. Retains tree-structure and still displays inter-cluster nodes nicely.\n",
    "\\\n",
    "\\\n",
    "Exp 86: Experiment to see whether BH accel. improves performance. It indeed does improve performance and the embeddings still look nice.\n",
    "\n",
    "#### Experiment 88, 89\n",
    "C_ELEGANS dataset using GaussianKL, BH accel. on/off, and adjusted learning rates\n",
    "\n",
    "#### Experiment 91\n",
    "HyperbolicKL on a tree-like dataset. Embeddings are strongly pushed towards the boundary\n",
    "\n",
    "#### Experiment 107, 108, 109\n",
    "HyperbolicKL on MNIST data. To see how differing learning rate lead to possibly different convergings. \n",
    "\\\n",
    "Wrong Gradient, No scale_fix, exact embedding (like Hunter's work/original paper)\n",
    "\\\n",
    "\\\n",
    "It seems that points are definitely pushed towards the edge.\n",
    "\n",
    "#### Experiment 110, 111, 112\n",
    "HyperbolicKL on MNIST data. Different learning rates. Same as (107, 108, 109) but not exact\n",
    "\\\n",
    "Wrong Gradient, No scale_fix, not exact embedding\n",
    "\\\n",
    "\\\n",
    "**Comparison between exact/non-exact**\n",
    "\\\n",
    "109 vs 110: Smallest lr comparison. Embeddings look very similar. Clusters are very dense\n",
    "\\\n",
    "107 vs 111: Medium lr comparison. Embeddings look very similar. Clusters are less dense\n",
    "\\\n",
    "108 vs 112: Largest lr comparison. Embeddings look very similar. Clusters are the most dense.\n",
    "\\\n",
    "\\\n",
    "It seems that lr. matters a lot in embedding quality. Too small or too large can produce too dense/crowded? clusters or embeddings. A sweet spot seems to be the medium lr (exag_factor * 100), or a lr around ~3.\n",
    "\n",
    "#### Experiment 113, 114, 115\n",
    "HyperbolicKL on C_ELEGANS dataset. Different learning rates.\n",
    "\\\n",
    "Wrong Gradient, No scale_fix, not exact\n",
    "\n",
    "\n",
    "## PC Experiments\n",
    "#### PC Experiment 35, 36\n",
    "HyperbolicKL on MNIST data. Different learning rates, (0.025 datasize)\n",
    "\\\n",
    "Correct Gradient, Scale_fix, exact\n",
    "\\\n",
    "\\\n",
    "Experiment to see what happens to embeddings when we differ the learning rates.\n",
    "Embeddings get pushed towards the edge. \n",
    "\n",
    "#### PC Experiment 37, 38\n",
    "HyperbolicKL on MNIST data. Different learning rates, (0.025 datasize)\n",
    "\\\n",
    "Correct Gradient, Scale_fix, not exact\n",
    "\\\n",
    "\\\n",
    "To see if embeddings look similar to 35, 36\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future work\n",
    "\n",
    "- Proper comparison of GaussianKL vs HyperbolicKL losses (on different data sets)\n",
    "- Extra investigation into BH accel. for GaussianKL\n",
    "- Possible \"crowding\" issues again with GaussianKL (although HyperbolicKL seems to have it too).\n",
    "  Clusters basically become point-like (for tree datasets), which is strange since it is not the case in experiment 38\n",
    "- Produce regular t-sne visualizations of Tree-like data\n",
    "- Quantitative comparison of gradients \n",
    "- Comparing cost function between gradients\n",
    "- Motivate why we should use the correct gradient; Compare \"wrong\" gradient with \"right gradient objectives, what is being minimized in either cases? Do they both minimize the same objective?\n",
    "- Writing; TU Delft writing center\n",
    "- Structure all the goals, findings. Connect the contributions and progress, experiments that backup the contributions. Have a logical ordering from hunter's thesis, wrong gradient, to gaussian gradient and backup/connect the steps together. Identify my contributions in this chain and note them down for the thesis.\n",
    "- Write down claims/goals explicitly, where is the story/thesis heading to. What are my claims?\n",
    "- Add quantitative measurements for NE methods to substantiate my claims\n",
    "\n",
    "- Time plan for this structure\n",
    "- Aggregate important information into a file for teams\n",
    "- Share overleaf link in teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis structure\n",
    "\n",
    "1. Introduction\n",
    "    - **Introduce hyperbolic embeddings**\n",
    "    Firsly we briefly introduce hyperbolic embeddings and its use cases. Explain connection between hierarchical relationships, tree/graph/network embeddings and hyperbolic space. Mention acceleration structure by Hunter.\n",
    "\n",
    "    - **Explain my contributions, where the thesis is heading** \\\n",
    "    Secondly we talk about correcting the gradient:\\\n",
    "    We introduce a correction to the gradient(s) used in other work. Hint at methods/experiment section for proper justification but briefly state justifications here.\n",
    "    \\\n",
    "    Then we talk about the Gaussian Gradient as a follow-up contribution for hyperbolic embeddings. Briefly explain why this contribution is significant.\n",
    "\n",
    "    <!-- - **Why would people care** \\\n",
    "    Other versions used use a wrong gradient (possibly they didn't fully derive the gradient but just took euclidean t-sne gradient and substituted some terms). \\\n",
    "    Wrong gradient may lead to false insights as results do not optimize the correct cost function. Or rather, we don't know, can't fully explain what is being optimized. Link to experiments/methods where we empricially show (through analysis of cost function progression) that the 2 gradients act differently.\n",
    "    -->\n",
    "\n",
    "    - **Challenges** \n",
    "    TBD...\n",
    "\n",
    "\n",
    "2. Related work (Straightforward)\n",
    "    - data visualization (tsne)\n",
    "    - Hyperbolic space & Visualizations\n",
    "    - Hunters' work, other hyperbolic visualization\n",
    "    - BH acceleration\n",
    "\n",
    "\n",
    "3. Background (Straightforward)\n",
    "    - Technical details about what is required to understand my thesis\n",
    "    - Hyperbolic space, t-sne/neighbour embedding methods, gradient descent\n",
    "\n",
    "\n",
    "4. Methods \\\n",
    "This section contains an overview of what I am contributing. Contains detailed reasons and motivations for what Im contributing (although empirical evidence is in the Experiments/Discussion sections).\n",
    "    - **Correct Gradient** \\\n",
    "        Justify the change to the correct gradient as with the following:\n",
    "        - Mathematical correctness argument, and bring into question what were even optimizing with the (wrong) gradient.\n",
    "        - A correct mathematical derivation of the gradient (appendix?)\n",
    "        - Empirical evidence of cost function convergence (to be elaborated on in Experiments section), to show that we are indeed not optimizing what we expect to be optimizing with the wrong gradient. (cost function)\n",
    "    \n",
    "    - **Tree-like data embedding** \\\n",
    "        Hyperbolic Space is supposed to embed trees (and graph/network-like) data well. (many references) \\\n",
    "        One contribution is an explicit experiment to test how trees are embedded using artificial tree-like data.\n",
    "        - Explain this idea with a some detail. Why do we care about embedding a tree. \n",
    "        - Emphasize how we use tree-like data as further evidence for the correct gradient, and t-sne/gaussian gradient contributions.\n",
    "        - Briefly go over how this data gets generated?\n",
    "        - Show some visualizations \n",
    "        - Use quantifiable quality metrics (in experiments & discussion)\n",
    "\n",
    "    - **Discuss limitations of t-sne distribution** \\\n",
    "        Justify this contribution step as follows:\n",
    "        - Show how both the correct (and incorrect, aka original gradient used) gradient push points towards the boundary. \\\n",
    "        Originally, the incorrect gradient seems to not do this, but if you keep the algorithm running, it will also converge to a similarily pushed out embedding.\n",
    "        - Explain why this is not desirable.\n",
    "        - Explain where this comes from (heavy tails of t-distribution, repulsive forces being too strong, the nature and limitations of using Poincare Disk for visualization)\n",
    "        - Visualization results & evaluation metric to further justify (in experiments & discussion)\n",
    "\n",
    "    - **Proposal of Gaussian distribution usage** \\\n",
    "        Motivate gaussian distribution usage: \n",
    "        - Origina SNE paper used gaussian\n",
    "        - t-distrib. was introduced for the crowding problem. Explain how in Hyperbolic space, this might be less of an issue (area/volume scales exponentially instead of linearly in 1D /quadratically in 2D)\n",
    "        - Gaussian has flatter tails, so forces aren't felt a smuch. This can be demonstrated mathematically.\n",
    "        - Visualization results & evaluation metric to further justify (in experiments & discussion)\n",
    "\n",
    "    - **Integrating everything with Hunter's work** \\\n",
    "        - Link everything back to the acceleration structure build by Hunter.\n",
    "        - Show that acceleration works equally as valid for Gaussian gradient.\n",
    "        - Maybe more??\n",
    "        \n",
    "\n",
    "5. Experiments \\\n",
    "    Talk about experiments that motivate this thesis, connect them to my contributions/goals. Experiment starts with a question targetting a contribution/goal, answer question using the experiment, relate it to the next experiment\n",
    "\n",
    "    - **E1: Correct Gradient justification experiments** \\\n",
    "    Firstly we compare hyperbolic t-sne correct vs. incorrect gradient:\n",
    "        1. Cost function convergence graphs for several datasets incl. tree data\n",
    "        2. Visual comparison of embeddings for several datasets \n",
    "\n",
    "    - **E2: Limitations of t-distrib. Gradient** \n",
    "        1. Show for (correct & wrong) gradient visualizations where embeddings are pushed out\n",
    "        2. Quality metrics of t-distrib. gradient \n",
    "    \n",
    "    - **E3: Improvents by Gaussian Gradient**\n",
    "        1. Show (corrected) gaussian embeddings. Use same datasets as in t-distrib. section\n",
    "        2. Quality metrics of gaussian gradient\n",
    "\n",
    "    - **E4: Tree-data embeddings** \\\n",
    "    Maybe this can be used to further highlight a limitation of the t-distrib. gradient. So we put this section between **E2** and **E3**. \n",
    "        1. Show tree-data embedding for t-distrib. and gaussian gradient. \\\n",
    "           Produce results for varying \"sized\" trees. (differing in depth. children etc...)\n",
    "        2. Quality metrics for this dataset\n",
    "    \n",
    "    - **E5: Acceleration structure for Gaussian gradient** \\\n",
    "    Investigate acceleration on Gaussian gradient embeddings? (TBD...)\n",
    "        1. Proceed same as Hunter did in his thesis (TBD...)\n",
    "\n",
    "\n",
    "6. Discussion\n",
    "    - Discuss experiments and connect them together to shape a strong motivation for my goal\n",
    "    - Focussed on content of thesis (methods/experiments)\n",
    "\n",
    "    - **Use E1 to justify Correct Gradient** \\\n",
    "    Here we elaborate and fully justify the use of the correct gradient over the wrong one using **E1**\n",
    "    We use the cost function graphs to highlight the problem with the wrong gradient (is not properly minimizing the objective in question), and show that the correct gradient does indeed minimize what we want. \n",
    "\n",
    "    - **E2 and t-distrib. limitations** \\\n",
    "    Here we use **E2** to convince the reader of the limitations of the t-distrib. gradient. We set the stage for the Gaussian gradient contribution. \n",
    "\n",
    "    - **E4: Tree-data justification** (maybe not as separate section) \\\n",
    "    Use tree-data embeddings to further justify Gaussian gradient over t-distrib. gradient. We can again use visuals, cost function comparisons, and quality retention metrics.\n",
    "    We can also justify this with related works that talk about tree/graph/network data embedding in Hyperbolic space.\n",
    "\n",
    "    - **E3: Gaussian gradient justification** \\\n",
    "    Now we show that the Gaussian gradient performs better in both visualization (and I assume also quality retention metrics).\n",
    "    We compare visuals, cost function experiment results, quality retention metrics to justify the usage of the Gaussian gradient. \n",
    "    \n",
    "\n",
    "7. Conclusion (TBD)\n",
    "    - Link experiments, discussion back to the original questions, goals, theme of the thesis. \n",
    "    - Present recommendations, risks, guidelines based on experiments/discussion\n",
    "    - Put things in context globally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General notes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start with Hunters' work. Hyperbolic Embeddings in general. Try to obtain some results that bring doubt to claims in those original papers?\n",
    "    - Wrong gradient vs Right gradient?\n",
    "    - What is being optimized wrt. wrong gradient?\n",
    "    - Is the actual cost function being minimized when we use the wrong gradient?\n",
    "    - What is happening to the cost function when we use the wrong gradient?\n",
    "    - How does this relate to the resulting embedding?\n",
    "Talk about related works (Poincare map, co-sne, h-sne) that also use the wrong gradient. \n",
    "\n",
    "\n",
    "    - What goes on when we use the correct gradient?\n",
    "    - Can we compare gradients? (think about this a bit more)\n",
    "    - Why do we care about a correct gradient? (Hyperbolic space, capturing hierarchical structure)\n",
    "    - Does the wrong gradient capture hierarchical structure?\n",
    "    - Why do we care about the correct gradient?\n",
    "    - TODO: Experiment with wrong gradient on various datasets\n",
    "\n",
    "Also go into core motivations behind Hyperbolic embeddings. Are we capable of capturing hierarchy (visually) using Hunters' work? If not, why not? How do we know we're not? \n",
    "\\\n",
    "\\\n",
    "What is the end goal of such work? What kind of visualizations would we like to obtain? Do we have baseline experiments that can affirm this? (HyperbolicKL variants on tree-like data)\n",
    "\n",
    "2. Lead insights, problems, \"mistakes\" from .1 to the focus of my thesis. \n",
    "    - What does Hunters' work etc.. fail to capture? What would we expect vs. what is being captured?\n",
    "    - \n",
    "\n",
    "3. My proposals to \"resolving\" issues/mistakes/problems arising from .1 and .2\n",
    "    - GaussianKL (and why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
