{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greenlight presentation notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current progress\n",
    "\n",
    "tldr: HyperbolicKL has some issues, we switch to GaussianKL to overcome them (hopefully)\n",
    "\n",
    "### HyperbolicKL notes\n",
    "I'm trying to figure out what exactly is going wrong when certain flags are set:\n",
    "1. Baseline experiment: (MNIST, wrong_grad, no scale_fix, HyperbolicKL, accel. on/off) \\\n",
    "   Works perfectly good. We get the results matching the paper (baseline)\n",
    "\n",
    "2. Experiment 2: (MNIST, wrong_grad, scale_fix, HyperbolicKL, acce. on/off) \\\n",
    "   Works fairly well. We get results similar to baseline. But it requires (much) more iterations to converge nicely.\n",
    "   This is due to the scaling of the inverse metric tensor term. Since it basically scales the gradients down consistently.\n",
    "\n",
    "3. Experiment 3: (MNIST, correct_gad, no scale_fix, HyperbolicKL, accel. on/off) \\\n",
    "   Takes much longer to converge properly. The extra d^H_ij term makes gradients (near) 0 for a long time.\n",
    "   Gradients get very large too at some point (embeddings overshoot the Disk)\n",
    "\n",
    "4. Experiment 4: (MNIST, correct_grad, scale_fix, HyperbolicKL, accel. on/off) \\\n",
    "   Takes even longer to converge properly. Now we have both d^H_ij (initially) and scale_fix scaling the gradients down. \\\n",
    "   However now gradients stay within reasonable amounts and don't explode off to large values. Presumably due to the scale_fix \\\n",
    "   These settings reproduce the result where points are pushed towards the boundary (conceptual sensible one)\n",
    "\n",
    "In points (3, 4) we get other issues. So the fact that gradients are very small means no progress is made for a long time, means the algorithm just stops.\n",
    "\\\n",
    "If we use both fixes (gradient, scale_fix) then we get the \"issue\" that points are pushed towards the boundary. \n",
    "\n",
    "To avoid early stopping increase **n_iter_check** flag in **opt_config** and manually change **n_iter_without_progress** in **solver.py**\n",
    "\n",
    "#### HyperbolicKL on Tree data\n",
    "Does not work very well. Often points don't even move from the center or other weirdness happens. I haven't tested this specific case much yet though.\n",
    "\n",
    "#### HyperbolicKL on Real data\n",
    "Pushes points towards the boundaries much more strongly than GaussianKL\n",
    "\\\n",
    "See experiment (73, 74)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperbolicKL to GaussianKL Motivation\n",
    "\n",
    "Gaussian has faster decreasing tails, resulting in forces being propegated out less, so embeddings are less likely to be pushed outwards towards the edge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianKL notes\n",
    "\n",
    "Some general notes:\n",
    "\n",
    "- size_tol parameter and the Hyperbolic variance really affect the embedding. This requires some hand-tuning\n",
    "- Early exaggeration \n",
    "- BH accel. numerical problems\n",
    "- Large tree depth problems\n",
    "- Crowding problem? Clusters being squished to points?\n",
    "- GaussianKL gradient: (2/var term and learning rate)\n",
    "\n",
    "#### GaussianKL on Tree dataset\n",
    "On the tree dataset, GaussianKL seems to produce much better results than HyperbolicKL. Although there are some issues.\n",
    "\n",
    "#### GaussianKL on Real datasets (MNIST, C_ELEGANS for now)\n",
    "On the real datasets, GaussianKL does not push embeddings towards the boundaries as much as HyperbolicKL. \n",
    "\\\n",
    "See experiment (71, 72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree dataset\n",
    "\n",
    "To properly test the notion that tree-like data can be nicely embedded in Hyperbolic Space, we turn to an artificially created tree-like dataset. \n",
    "\\\n",
    "I basically wrote some code that generates a Distance **D** and affinity **V** Matrix such that its entries reflect a tree-like ordering of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "Some notable experiments and analysis:\n",
    "\n",
    "#### Experiment 38 (also 68)\n",
    "This experiment nicely showcases a tree embedding using GaussianKL. However, this experiment had an extra grad \\*= 4 term in the cost function (leftover when I coped over the HyperbolicKL code) that caused the embedding to converge to this specific visualization.\n",
    "\\\n",
    "\\\n",
    "If this term is removed, the clusters are more dense and converge to points. (experiment 66)\n",
    "\\\n",
    "\\\n",
    "However, we can regain clusters by adjusting the learning rate (this basically mimics the grad \\*=4 mistake).\n",
    "It seems that adjusting the learning rate will give us \"better\" (subjective and context-dependent) embeddings.\n",
    "\n",
    "\n",
    "#### Experiment 73. 74\n",
    "Showcases HyperbolicKL on MNIST. Points are pushed along the boundary very strongly\n",
    "\n",
    "#### Experiment 71, 72\n",
    "Showcases GaussianKL on MNIST. Points are much less pushed toward the center and the embedding actually takes shape inside the visible part of the disk.\n",
    "\n",
    "#### Experiment 83, 84\n",
    "Attempt to reproduce 38, 68 without the grad \\*= 4 term, but by adjusting the learning rate since it achieves basically the same result. This attempt was succesful. Therefore learning rate really affects the embeddings.\n",
    "\n",
    "#### Experiment 85, 86\n",
    "Experiment that showcases how adjusting learning rate also improves embeddings for larger trees. Retains tree-structure and still displays inter-cluster nodes nicely.\n",
    "\\\n",
    "\\\n",
    "Exp 86: Experiment to see whether BH accel. improves performance. It indeed does improve performance and the embeddings still look nice.\n",
    "\n",
    "#### Experiment 88, 89\n",
    "C_ELEGANS dataset using GaussianKL, BH accel. on/off, and adjusted learning rates\n",
    "\n",
    "#### Experiment 91\n",
    "HyperbolicKL on a tree-like dataset. Embeddings are strongly pushed towards the boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future work\n",
    "\n",
    "- Proper comparison of GaussianKL vs HyperbolicKL losses (on different data sets)\n",
    "- Extra investigation into BH accel. for GaussianKL\n",
    "- Possible \"crowding\" issues again with GaussianKL (although HyperbolicKL seems to have it too).\n",
    "  Clusters basically become point-like (for tree datasets), which is strange since it is not the case in experiment 38\n",
    "- Produce regular t-sne visualizations of Tree-like data\n",
    "- Quantitative comparison of gradients \n",
    "- Comparing cost function between gradients\n",
    "- Motivate why we should use the correct gradient; Compare \"wrong\" gradient with \"right gradient objectives, what is being minimized in either cases? Do they both minimize the same objective?\n",
    "- Writing; TU Delft writing center\n",
    "- Structure all the goals, findings. Connect the contributions and progress, experiments that backup the contributions. Have a logical ordering from hunter's thesis, wrong gradient, to gaussian gradient and backup/connect the steps together. Identify my contributions in this chain and note them down for the thesis.\n",
    "- Write down claims/goals explicitly, where is the story/thesis heading to. What are my claims?\n",
    "- Add quantitative measurements for NE methods to substantiate my claims\n",
    "\n",
    "- Time plan for this structure\n",
    "- Aggregate important information into a file for teams\n",
    "- Share overleaf link in teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis structure\n",
    "\n",
    "1. Introduction\n",
    "    - Explain my contributions, where the thesis is heading\n",
    "    - Why would people care\n",
    "    - Challenges\n",
    "2. Related work\n",
    "    - data visualization (tsne)\n",
    "    - Hyperbolic space visualizations\n",
    "    - Hunters' work, other hyperbolic visualization\n",
    "    - BH acceleration\n",
    "3. Background\n",
    "    - Technical details about what is required to understand my thesis\n",
    "    - Hyperbolic space, t-sne, gradient descent, math etc..\n",
    "4. Methods\n",
    "    - What did I do on a conceptual level. \n",
    "    - What are my proposed solutions, adjustments, methods\n",
    "    - The conceptual/theoretical part, motivations, derivations\n",
    "5. Experiments\n",
    "    - Experiments that motivate this thesis, connect them to my contributions/goals\n",
    "    - Experiment starts with a question targetting a contribution/goal, answer question using the experiment, relate it to the next experiment\n",
    "6. Discussion\n",
    "    - Discuss experiments and connect them together to shape a strong motivation for my goal\n",
    "    - Focussed on content of thesis (methods/experiments)\n",
    "7. Conclusion\n",
    "    - Link experiments, discussion back to the original questions, goals, theme of the thesis. \n",
    "    - Present recommendations, risks, guidelines based on experiments/discussion\n",
    "    - Put things in context globally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis storyline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start with Hunters' work. Hyperbolic Embeddings in general. Try to obtain some results that bring doubt to claims in those original papers?\n",
    "    - Wrong gradient vs Right gradient?\n",
    "    - What is being optimized wrt. wrong gradient?\n",
    "    - Is the actual cost function being minimized when we use the wrong gradient?\n",
    "    - What is happening to the cost function when we use the wrong gradient?\n",
    "    - How does this relate to the resulting embedding?\n",
    "Talk about related works (Poincare map, co-sne, h-sne) that also use the wrong gradient. \n",
    "\n",
    "\n",
    "    - What goes on when we use the correct gradient?\n",
    "    - Can we compare gradients? (think about this a bit more)\n",
    "    - Why do we care about a correct gradient? (Hyperbolic space, capturing hierarchical structure)\n",
    "    - Does the wrong gradient capture hierarchical structure?\n",
    "    - Why do we care about the correct gradient?\n",
    "    - TODO: Experiment with wrong gradient on various datasets\n",
    "\n",
    "Also go into core motivations behind Hyperbolic embeddings. Are we capable of capturing hierarchy (visually) using Hunters' work? If not, why not? How do we know we're not? \n",
    "\\\n",
    "\\\n",
    "What is the end goal of such work? What kind of visualizations would we like to obtain? Do we have baseline experiments that can affirm this? (HyperbolicKL variants on tree-like data)\n",
    "\n",
    "2. Lead insights, problems, \"mistakes\" from .1 to the focus of my thesis. \n",
    "    - What does Hunters' work etc.. fail to capture? What would we expect vs. what is being captured?\n",
    "    - \n",
    "\n",
    "3. My proposals to \"resolving\" issues/mistakes/problems arising from .1 and .2\n",
    "    - GaussianKL (and why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
