{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "from pathlib import Path \n",
    "import traceback\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperbolicTSNE imports\n",
    "from hyperbolicTSNE import Datasets, load_data\n",
    "from hyperbolicTSNE import Datasets, SequentialOptimizer, initialization, HyperbolicTSNE\n",
    "from hyperbolicTSNE.cost_functions_ import HyperbolicKL, GaussianKL\n",
    "from hyperbolicTSNE.util import find_last_embedding, opt_config, initialize_logger, save_experiment_results, next_experiment_folder_id, GaussianKL_results, HyperbolicKL_results\n",
    "from hyperbolicTSNE.data_loaders import load_mnist\n",
    "from hyperbolicTSNE.hd_mat_ import hd_matrix\n",
    "from hyperbolicTSNE.visualization import plot_poincare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General initialization values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = \"temp/poincare/\"  # path for saving embedding snapshots\n",
    "grad_path = \"temp/grad/\"     # NOTE: We will manually calculate the gradients\n",
    "data_home = \"datasets\"\n",
    "\n",
    "experiments_folder = \"./experiment_results/\"\n",
    "exp_id = next_experiment_folder_id(experiments_folder)\n",
    "\n",
    "# Experiment settings\n",
    "seed = 42\n",
    "correct_grads = [True]                   # NOTE: Recompile with correct flag (GRAD_FIX flag)\n",
    "exacts = [False]                          # NOTE: Exact computation or BH estimation of gradient\n",
    "grad_scale_fixes = [True]          # Whether we multiply the gradient by the inverse metric tensor of hyperbolic space or not\n",
    "                                          # Note that the correct hyperoblic gradient has an inverse metric tensor factor\n",
    "\n",
    "# Simple experiment with no exaggeration\n",
    "exaggeration_factor = 12\n",
    "ex_iterations = 2000\n",
    "main_iterations = 15000\n",
    "\n",
    "# Cost functions\n",
    "cfs = [HyperbolicKL]\n",
    "\n",
    "# Other params\n",
    "size_tol = 0.999\n",
    "max_dist_H = 0\n",
    "max_dist = 0    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization values for the non custom datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [Datasets.MNIST]\n",
    "num_points = [0.1]\n",
    "perplexities = [25]\n",
    "pca_components = [50]                              # Whether to use pca initialization of high dim. data or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "# NOTE: Exerpimental details for unfixed datasets etc..\n",
    "# experiment_details = list(product(datasets, num_points, perplexities, pca_components, cfs, correct_grads, exacts))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# NOTE: Below, experimental setup details for when we have a fixed dataset and dataset parameters \n",
    "# NOTE: But have different cost functions and cost function settings\n",
    "experiment_details_2 = list(product(cfs, correct_grads, exacts, grad_scale_fixes))\n",
    "\n",
    "for experiment in experiment_details_2:\n",
    "    print(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[0]\n",
    "num_p = num_points[0]\n",
    "perp = perplexities[0]\n",
    "pca = pca_components[0]\n",
    "\n",
    "\n",
    "# (0) Load data\n",
    "dataX, dataLabels, D, V, *rest = load_data(\n",
    "    dataset, \n",
    "    data_home=data_home, \n",
    "    pca_components=pca,\n",
    "    random_state=seed, \n",
    "    to_return=\"X_labels_D_V\",\n",
    "    hd_params={\"perplexity\": perp}, \n",
    "    sample=num_p, \n",
    "    knn_method=\"hnswlib\"  # we use an approximation of high-dimensional neighbors to speed up computations\n",
    ")\n",
    "\n",
    "\n",
    "# (1) Compute initial embedding in Poincare disk (PCA embedding)\n",
    "X_embedded = initialization(\n",
    "    n_samples=dataX.shape[0], \n",
    "    n_components=2,\n",
    "    X=dataX,\n",
    "    random_state=seed,\n",
    "    method=\"random\",\n",
    "    init_scale=1e-4         # spread out initializations more\n",
    ") \n",
    "\n",
    "print(\"Data shape: \", X_embedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for running a bunch of experiments at the same time (types of experiments defined above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments\n",
    "# for (dataset, num_p, perp, pca, cf, correct_grad, exact) in experiment_details:\n",
    "for (cf, correct_grad, exact, grad_scale_fix) in experiment_details_2:\n",
    "    print(f\"\\n[Experiment: {exp_id}] \\n dataset: {dataset}, num_p: {num_p}, perp: {perp},pca: {pca}, correct_grad: {correct_grad}, exact: {exact}, scale_fix: {grad_scale_fix}\\n\")\n",
    "    \n",
    "    # NOTE: Uncomment if we decide to go with multiple datasets etc..\n",
    "    # # (0) Load data\n",
    "    # dataX, dataLabels, D, V, *rest = load_data(\n",
    "    #     dataset, \n",
    "    #     data_home=data_home, \n",
    "    #     pca_components=pca,\n",
    "    #     random_state=seed, \n",
    "    #     to_return=\"X_labels_D_V\",\n",
    "    #     hd_params={\"perplexity\": perp}, \n",
    "    #     sample=num_p, \n",
    "    #     knn_method=\"hnswlib\"  # we use an approximation of high-dimensional neighbors to speed up computations\n",
    "    # )\n",
    "\n",
    " \n",
    "    # # (1) Compute initial embedding in Poincare disk (PCA embedding)\n",
    "    X_embedded = initialization(\n",
    "        n_samples=dataX.shape[0], \n",
    "        n_components=2,\n",
    "        X=dataX,\n",
    "        random_state=seed,\n",
    "        method=\"random\",\n",
    "        init_scale=1e-4         # spread out initializations more\n",
    "    ) \n",
    "\n",
    "    # Set learning rates\n",
    "    if cf == GaussianKL:        \n",
    "        hyp_var = 0.2\n",
    "        learning_rate = dataX.shape[0] / (exaggeration_factor * 10) * hyp_var\n",
    "\n",
    "    else:\n",
    "        learning_rate = dataX.shape[0] / (exaggeration_factor * 10)\n",
    "\n",
    "    print(f\"learning rate: {learning_rate}\")\n",
    "\n",
    "    # Initialize configs\n",
    "    opt_conf = opt_config(cf, learning_rate, exaggeration_factor, ex_iterations, main_iterations, exact=exact, vanilla=True, grad_scale_fix=grad_scale_fix, grad_fix=correct_grad, max_no_progress=int(main_iterations))\n",
    "    opt_params = SequentialOptimizer.sequence_poincare(**opt_conf) \n",
    "    log_path_cf = log_path + f\"cf_{cf.class_str()}/correct_grad_{correct_grad}/\"\n",
    "    grad_path_grad = grad_path + f\"cf_{cf.class_str()}/correct_grad_{correct_grad}/\"\n",
    "\n",
    "\n",
    "    # (3) Update config params using computed variance  \n",
    "    opt_params, opt_conf = initialize_logger(opt_params, opt_conf, log_path_cf, grad_path_grad)\n",
    "    opt_params[\"cf_params\"].update({\"grad_fix\" : correct_grad})     # So the cost function knows which gradient to use\n",
    "    if cf == GaussianKL: opt_params[\"cf_params\"].update({\"var\" : hyp_var})     # GaussianKL variance for q_ij\n",
    "\n",
    "\n",
    "    # (4) Set up t-SNE object and run\n",
    "    htsne = HyperbolicTSNE(\n",
    "        init=X_embedded, \n",
    "        n_components=2, \n",
    "        metric=\"precomputed\",\n",
    "        verbose=1, \n",
    "        opt_method=SequentialOptimizer,         # the optimizater we use\n",
    "        opt_params=opt_params              # the parameters for the optimizers\n",
    "    )\n",
    "\n",
    "    # Compute embedding:\n",
    "    try:\n",
    "        hyperbolicEmbedding = htsne.fit_transform((D, V))\n",
    "        \n",
    "    except ValueError:\n",
    "        hyperbolicEmbedding = find_last_embedding(log_path)\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "    emb_fig = plot_poincare(hyperbolicEmbedding, dataLabels, dataLabels)\n",
    "\n",
    "    # (6) Store experiment results\n",
    "    # folder to save results to\n",
    "    save_folder = f\"./experiment_results/experiment_{exp_id}/\"   \n",
    "    exp_id += 1    \n",
    "\n",
    "    # dictionary containing relevant details of this experiment\n",
    "    optim_procedure = \"Vanilla SGD\"\n",
    "    description = \"HyperbolicKL MNIST, with more data, more iterations. Aim to get cf values\"\n",
    "\n",
    "    if cf == GaussianKL:\n",
    "        exp_data = GaussianKL_results(dataset.name, htsne, ex_iterations, learning_rate, perp, num_p, pca,\n",
    "                       main_iterations, cf, hyp_var, size_tol, max_dist_H, max_dist, correct_grad,\n",
    "                       grad_scale_fix, exact, exaggeration_factor, optim_procedure, description)\n",
    "        \n",
    "    elif cf == HyperbolicKL:\n",
    "        exp_data = HyperbolicKL_results(dataset.name, htsne, ex_iterations, learning_rate, perp, num_p, pca,\n",
    "                       main_iterations, cf, correct_grad, grad_scale_fix, exact, \n",
    "                       exaggeration_factor, optim_procedure, description)\n",
    "\n",
    "    animation_step = 25\n",
    "    save_experiment_results(save_folder, None, emb_fig, opt_params, dataLabels, exp_data, hyperbolicEmbedding, htsne=htsne, step=animation_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Hyperbolic Variance\n",
    "\n",
    "First we compute how many Euclidean variances wide the maximum distance between 2 datapoints is:\n",
    "$$ c = \\frac{D_{max}}{\\sigma^2_E} $$\n",
    "\n",
    "By setting the Hyperbolic variance to (below) we gain a value for the Hyperbolic variance that hopefully causes the Hyperbolic embeddings to stay reasonably within the visualizable part of the Poincare Disk.\n",
    "$$ \\sigma^2_D = \\frac{d^H(-0.99, 0.99)}{c}$$\n",
    "\n",
    "[TODO: Expand on elaboration]\n",
    "This is because I suspect that by relating the variance (like in above), the embedding process will cause embeddings to not spread out too much. Since when we are matching the probability distributions, the probabilities will fall off quick as we get multiple variances away. By relating the variance to the maximum distance we want out of our embedding, and at the same time having a $$ c $$ that encodes the high dimensional relatonship between embedding distance size and variance, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store experiment results and data into a dedicated folde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical analysis of gradients\n",
    "\n",
    "Investigate the probability distributions of high/low dim. respectively between the 2 gradients\n",
    "Investigate ratio of gradients between correct/wrong gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperbolic distance between y1, y2\n",
    "def hyp_dist(y1, y2):\n",
    "    diff = y1 - y2\n",
    "    num = diff.dot(diff)                         # numerator\n",
    "    denum = (1 - y1.dot(y1)) * (1 - y2.dot(y2))  # denumerator\n",
    "    dist = np.arccosh(1 + 2 * (num / denum))\n",
    "\n",
    "    return dist\n",
    "\n",
    "\n",
    "# Compute the probability distribution associated with our embedding.\n",
    "# Low dimensional affinities, computed from embeddings \n",
    "def compute_affinities(embedding_data):\n",
    "    Q = np.zeros((embedding_data.shape[0], embedding_data.shape[0]))\n",
    "    for i in range(Q.shape[0]):\n",
    "        for j in range(i + 1, Q.shape[1]):\n",
    "            dist = hyp_dist(embedding_data[i], embedding_data[j])       # hyp. distance\n",
    "            dist = 1. / (1 + dist**2)                                   # t-distrib. \"distance\"\n",
    "            Q[i][j] = dist\n",
    "            Q[j][i] = dist\n",
    "\n",
    "    # Normalize distances to probabilities\n",
    "    # return Q / Q.sum()\n",
    "    return Q / Q.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recomputing probability matrices\n",
    "P = V.toarray()                         # high dim. affinity\n",
    "Q = compute_affinities(data[\"Emb\"])     # low dim. affinity\n",
    "\n",
    "print(P[0])\n",
    "print()\n",
    "print(Q[0])\n",
    "\n",
    "print(P - Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of behaviour at beginning (early iterations)\n",
    "\n",
    "1. hyp. distances are small $d^{H}_{ij}$ approaches 0 (since $cosh^{-1} goes to 0), since its argument goes to 1.\n",
    "\n",
    "2. This means t-distrib. probability goes to 1, $(1 + (d^{H}_{ij})^2)^{-1}$ goes to 1.\n",
    "So once normalized, all probabilities $q^{H}_{ij}$ are almost the same for all $ij$, so we basically have an uniform distribution for $q^{H}_{ij}$\n",
    "\n",
    "3. Why are the wrong gradients so much bigger?\n",
    "\n",
    "Correct Gradient expression:\n",
    "$$ 4  \\sum_{j} (p_{ij} - q^{H}_{ij}) (1 + (d^{H}_{ij})^2)^{-1}  d^{H}_{ij} \\frac{d^{H}_{ij}}{y_i}$$\n",
    "\n",
    "\\\n",
    "Wrong Gradient expresion:\n",
    "$$ 4  \\sum_{j} (p_{ij} - q^{H}_{ij}) (1 + (d^{H}_{ij})^2)^{-1} \\frac{d^{H}_{ij}}{y_i}$$\n",
    "\n",
    "\\\n",
    "Because in early iterations, $ d^{H}_{ij} $ is approximately 0, the correct gradient is very small, until points haved moved sufficiently for this term to increase in value.\n",
    "\n",
    "Can this possibly replace early exaggeration? Maybe.. \n",
    "At the beginning, points are updated in very small amounts. But this applies to both attractive and repulsive forces.\n",
    "\n",
    "At some points, points are \"distant\" enough, for $ d^{H}_{ij} $ to not matter much anymore. \n",
    "\n",
    "$ d^{H}_{ij} $ goes up a lot (to possibly infinity) for larger distances. This means that forces between distant points get heavily amplified. Is this an issue though? Since very distant forces should be more or less 0?\n",
    "\n",
    "4. Is the use of the t-distribution justified in hyperbolic space?\n",
    "Is it making repellant points feel a repellant force for way too far distances?\n",
    "What do p, q look like for points far away? Is the heavy-tailedness a problem in hyperbolic space?\n",
    "\n",
    "Despite large distances, $ q^{H}_{ij} $ is never able to reach the \"correct\" probabilities due to the use of the t-distribution? Since the heavy tails requires too big of distances for things to converge properly. Hence points keep getting pushed out? \n",
    "\n",
    "We can try using a regular gaussian distribution for $ q^{H}_{ij} $. Rederive the gradient, and experiment with that.\n",
    "But do we then need to take the symmetric version into account?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delft_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
