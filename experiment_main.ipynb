{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "from pathlib import Path \n",
    "import traceback\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/haoranxia/Thesis-Delft/hyperbolic-tsne'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperbolicTSNE imports\n",
    "from hyperbolicTSNE import Datasets, load_data\n",
    "from hyperbolicTSNE import Datasets, SequentialOptimizer, initialization, HyperbolicTSNE\n",
    "from hyperbolicTSNE.cost_functions_ import HyperbolicKL, GaussianKL\n",
    "from hyperbolicTSNE.util import find_last_embedding, opt_config, initialize_logger, save_experiment_results, next_experiment_folder_id, GaussianKL_results, HyperbolicKL_results\n",
    "from hyperbolicTSNE.data_loaders import load_mnist\n",
    "from hyperbolicTSNE.hd_mat_ import hd_matrix\n",
    "from hyperbolicTSNE.visualization import plot_poincare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General initialization values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = \"temp/poincare/\"  # path for saving embedding snapshots\n",
    "grad_path = \"temp/grad/\"     # NOTE: We will manually calculate the gradients\n",
    "data_home = \"datasets\"\n",
    "\n",
    "experiments_folder = \"./experiment_results/\"\n",
    "exp_id = next_experiment_folder_id(experiments_folder)\n",
    "\n",
    "# Experiment settings\n",
    "seed = 42\n",
    "correct_grads = [True]                   # NOTE: Recompile with correct flag (GRAD_FIX flag)\n",
    "exacts = [False]                          # NOTE: Exact computation or BH estimation of gradient\n",
    "grad_scale_fixes = [True, False]          # Whether we multiply the gradient by the inverse metric tensor of hyperbolic space or not\n",
    "                                          # Note that the correct hyperoblic gradient has an inverse metric tensor factor\n",
    "\n",
    "# Simple experiment with no exaggeration\n",
    "exaggeration_factor = 12\n",
    "ex_iterations = 1000\n",
    "main_iterations = 15000\n",
    "\n",
    "# Cost functions\n",
    "cfs = [HyperbolicKL]\n",
    "\n",
    "# Other params\n",
    "size_tol = 0.999\n",
    "max_dist_H = 0\n",
    "max_dist = 0    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization values for the non custom datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [Datasets.MNIST]\n",
    "num_points = [0.1]\n",
    "perplexities = [25]\n",
    "pca_components = [50]                              # Whether to use pca initialization of high dim. data or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'hyperbolicTSNE.cost_functions_.HyperbolicKL'>, False, False, False)\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "# NOTE: Exerpimental details for unfixed datasets etc..\n",
    "# experiment_details = list(product(datasets, num_points, perplexities, pca_components, cfs, correct_grads, exacts))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# NOTE: Below, experimental setup details for when we have a fixed dataset and dataset parameters \n",
    "# NOTE: But have different cost functions and cost function settings\n",
    "experiment_details_2 = list(product(cfs, correct_grads, exacts, grad_scale_fixes))\n",
    "\n",
    "for experiment in experiment_details_2:\n",
    "    print(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  (7000, 2)\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets[0]\n",
    "num_p = num_points[0]\n",
    "perp = perplexities[0]\n",
    "pca = pca_components[0]\n",
    "\n",
    "\n",
    "# (0) Load data\n",
    "dataX, dataLabels, D, V, *rest = load_data(\n",
    "    dataset, \n",
    "    data_home=data_home, \n",
    "    pca_components=pca,\n",
    "    random_state=seed, \n",
    "    to_return=\"X_labels_D_V\",\n",
    "    hd_params={\"perplexity\": perp}, \n",
    "    sample=num_p, \n",
    "    knn_method=\"hnswlib\"  # we use an approximation of high-dimensional neighbors to speed up computations\n",
    ")\n",
    "\n",
    "\n",
    "# (1) Compute initial embedding in Poincare disk (PCA embedding)\n",
    "X_embedded = initialization(\n",
    "    n_samples=dataX.shape[0], \n",
    "    n_components=2,\n",
    "    X=dataX,\n",
    "    random_state=seed,\n",
    "    method=\"random\",\n",
    "    init_scale=1e-4         # spread out initializations more\n",
    ") \n",
    "\n",
    "print(\"Data shape: \", X_embedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for running a bunch of experiments at the same time (types of experiments defined above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Experiment: 106] \n",
      " dataset: Datasets.MNIST, num_p: 0.1, perp: 25,pca: 50, correct_grad: False, exact: False, scale_fix: False\n",
      "\n",
      "learning rate: 0.5833333333333334\n",
      "Please note that `empty_sequence` uses the KL divergence with Barnes-Hut approximation (angle=0.5) by default.\n",
      "config: {'cf': <class 'hyperbolicTSNE.cost_functions_.HyperbolicKL'>, 'learning_rate_ex': 0.5833333333333334, 'learning_rate_main': 0.5833333333333334, 'exaggeration': 12, 'exaggeration_its': 1000, 'gradientDescent_its': 15000, 'vanilla': True, 'momentum_ex': 0.5, 'momentum': 0.2, 'exact': False, 'area_split': False, 'n_iter_check': 2000, 'size_tol': 0.999, 'grad_scale_fix': False, 'grad_fix': False, 'n_iter_without_progress': 2000, 'min_grad_norm': 1e-10}\n",
      "[HyperbolicTSNE] Received iterable as input. It should have len=2 and contain (D=None, V=None)\n",
      "[hd_mat] Warning: There is nothing to do with given parameters. Returning given D and V\n",
      "[gradient_descent] Warning: because of logging, the cf will be computed at every iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gradient Descent error: 95.42662 grad_norm: 8.43489e-01:  16%|█▋        | 163/1000 [00:12<01:03, 13.19it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Compute embedding:\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     hyperbolicEmbedding \u001b[38;5;241m=\u001b[39m \u001b[43mhtsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     hyperbolicEmbedding \u001b[38;5;241m=\u001b[39m find_last_embedding(log_path)\n",
      "File \u001b[0;32m~/Thesis-Delft/hyperbolic-tsne/hyperbolicTSNE/hyperbolic_tsne_.py:245\u001b[0m, in \u001b[0;36mHyperbolicTSNE.fit_transform\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    229\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m        Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m     X_embedded, cf, runtime, its \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m X_embedded\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcf \u001b[38;5;241m=\u001b[39m cf \n",
      "File \u001b[0;32m~/Thesis-Delft/hyperbolic-tsne/hyperbolicTSNE/hyperbolic_tsne_.py:224\u001b[0m, in \u001b[0;36mHyperbolicTSNE._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_method(\n\u001b[1;32m    221\u001b[0m     Y0\u001b[38;5;241m=\u001b[39mX_embedded, V\u001b[38;5;241m=\u001b[39mV, n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components, other_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_params, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# Compute embeddings\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m X_embedded, cf, runtime, its \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_embedded, cf, runtime, its\n",
      "File \u001b[0;32m~/Thesis-Delft/hyperbolic-tsne/hyperbolicTSNE/optimizer_.py:168\u001b[0m, in \u001b[0;36mSequentialOptimizer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m se[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Start: logging\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcf_val, its \u001b[38;5;241m=\u001b[39m \u001b[43mse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mV\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcf_params\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msequential_opt_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msolver_counter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m solver_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# End: logging\u001b[39;00m\n",
      "File \u001b[0;32m~/Thesis-Delft/hyperbolic-tsne/hyperbolicTSNE/solver_.py:270\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(y0, cf, cf_params, start_it, n_iter, n_iter_check, n_iter_without_progress, threshold_cf, threshold_its, threshold_check_size, momentum, learning_rate, min_gain, vanilla, min_grad_norm, error_tol, size_tol, verbose, rescale, n_iter_rescale, gradient_mask, grad_scale_fix, logging_dict, logging_key)\u001b[0m\n\u001b[1;32m    267\u001b[0m compute_error \u001b[38;5;241m=\u001b[39m check_convergence \u001b[38;5;129;01mor\u001b[39;00m check_threshold \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m n_iter \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_error \u001b[38;5;129;01mor\u001b[39;00m logging:  \u001b[38;5;66;03m# TODO: add different levels of logging to avoid bottlenecks\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m     error, grad \u001b[38;5;241m=\u001b[39m \u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcf_params\u001b[49m\u001b[43m)\u001b[49m       \u001b[38;5;66;03m# NOTE: This is where we call the cost function\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cf, HyperbolicKL):\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;66;03m# New Fix; Multiply gradient by inverse metric tensor\u001b[39;00m\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m grad_scale_fix:\n",
      "File \u001b[0;32m~/Thesis-Delft/hyperbolic-tsne/hyperbolicTSNE/cost_functions_.py:217\u001b[0m, in \u001b[0;36mHyperbolicKL.obj_grad\u001b[0;34m(self, Y, V, grad_fix)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj, grad\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbarnes-hut\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m     obj, grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_obj_bh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_fix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj, grad\n",
      "File \u001b[0;32m~/Thesis-Delft/hyperbolic-tsne/hyperbolicTSNE/cost_functions_.py:306\u001b[0m, in \u001b[0;36mHyperbolicKL._obj_bh\u001b[0;34m(self, Y, V, n_samples, grad_fix)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_obj_bh\u001b[39m(\u001b[38;5;28mself\u001b[39m, Y, V, n_samples, grad_fix):\n\u001b[1;32m    288\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Approximate computation of the KL Divergence.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m        Array (n_samples x n_components) with KL Divergence gradient values.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grad_bh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_fix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Thesis-Delft/hyperbolic-tsne/hyperbolicTSNE/cost_functions_.py:336\u001b[0m, in \u001b[0;36mHyperbolicKL._grad_bh\u001b[0;34m(self, Y, V, n_samples, grad_fix, save_timings)\u001b[0m\n\u001b[1;32m    334\u001b[0m grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(Y\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mctypes\u001b[38;5;241m.\u001b[39mc_double)\n\u001b[1;32m    335\u001b[0m timings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m4\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mctypes\u001b[38;5;241m.\u001b[39mc_float)\n\u001b[0;32m--> 336\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_V\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mangle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbose\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdegrees_of_freedom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_threads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43marea_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marea_split\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_fix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_fix\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m    351\u001b[0m grad \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run experiments\n",
    "# for (dataset, num_p, perp, pca, cf, correct_grad, exact) in experiment_details:\n",
    "for (cf, correct_grad, exact, grad_scale_fix) in experiment_details_2:\n",
    "    print(f\"[Experiment: {exp_id}] \\n dataset: {dataset}, num_p: {num_p}, perp: {perp},pca: {pca}, correct_grad: {correct_grad}, exact: {exact}, scale_fix: {grad_scale_fix}\\n\")\n",
    "    \n",
    "    # NOTE: Uncomment if we decide to go with multiple datasets etc..\n",
    "    # # (0) Load data\n",
    "    # dataX, dataLabels, D, V, *rest = load_data(\n",
    "    #     dataset, \n",
    "    #     data_home=data_home, \n",
    "    #     pca_components=pca,\n",
    "    #     random_state=seed, \n",
    "    #     to_return=\"X_labels_D_V\",\n",
    "    #     hd_params={\"perplexity\": perp}, \n",
    "    #     sample=num_p, \n",
    "    #     knn_method=\"hnswlib\"  # we use an approximation of high-dimensional neighbors to speed up computations\n",
    "    # )\n",
    "\n",
    " \n",
    "    # # (1) Compute initial embedding in Poincare disk (PCA embedding)\n",
    "    X_embedded = initialization(\n",
    "        n_samples=dataX.shape[0], \n",
    "        n_components=2,\n",
    "        X=dataX,\n",
    "        random_state=seed,\n",
    "        method=\"random\",\n",
    "        init_scale=1e-4         # spread out initializations more\n",
    "    ) \n",
    "\n",
    "    # Initialize config and parameters\n",
    "    if cf == GaussianKL:        \n",
    "        hyp_var = 0.2\n",
    "        learning_rate = dataX.shape[0] / (exaggeration_factor * 10) * hyp_var\n",
    "\n",
    "    else:\n",
    "        learning_rate = dataX.shape[0] / (exaggeration_factor * 1000)\n",
    "\n",
    "    print(f\"learning rate: {learning_rate}\")\n",
    "\n",
    "\n",
    "    opt_conf = opt_config(cf, learning_rate, exaggeration_factor, ex_iterations, main_iterations, exact=exact, vanilla=True, grad_scale_fix=grad_scale_fix, grad_fix=correct_grad)\n",
    "    opt_params = SequentialOptimizer.sequence_poincare(**opt_conf) \n",
    "    log_path_cf = log_path + f\"cf_{cf.class_str()}/correct_grad_{correct_grad}/\"\n",
    "    grad_path_grad = grad_path + f\"cf_{cf.class_str()}/correct_grad_{correct_grad}/\"\n",
    "\n",
    "\n",
    "    # (3) Update config params using computed variance  \n",
    "    opt_params, opt_conf = initialize_logger(opt_params, opt_conf, log_path_cf, grad_path_grad)\n",
    "    opt_params[\"cf_params\"].update({\"grad_fix\" : correct_grad})     # So the cost function knows which gradient to use\n",
    "    if cf == GaussianKL: opt_params[\"cf_params\"].update({\"var\" : hyp_var})     # GaussianKL variance for q_ij\n",
    "\n",
    "\n",
    "    # (4) Set up t-SNE object and run\n",
    "    htsne = HyperbolicTSNE(\n",
    "        init=X_embedded, \n",
    "        n_components=2, \n",
    "        metric=\"precomputed\",\n",
    "        verbose=1, \n",
    "        opt_method=SequentialOptimizer,         # the optimizater we use\n",
    "        opt_params=opt_params              # the parameters for the optimizers\n",
    "    )\n",
    "\n",
    "    # Compute embedding:\n",
    "    try:\n",
    "        hyperbolicEmbedding = htsne.fit_transform((D, V))\n",
    "        \n",
    "    except ValueError:\n",
    "        hyperbolicEmbedding = find_last_embedding(log_path)\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "    emb_fig = plot_poincare(hyperbolicEmbedding, dataLabels, dataLabels)\n",
    "\n",
    "    # (6) Store experiment results\n",
    "    # folder to save results to\n",
    "    save_folder = f\"./experiment_results/experiment_{exp_id}/\"   \n",
    "    exp_id += 1    \n",
    "\n",
    "    # dictionary containing relevant details of this experiment\n",
    "    optim_procedure = \"Vanilla SGD\"\n",
    "    description = \"HyperbolicKL MNIST, with more data, more iterations, wrong gradient, to see if it converges to correct gradient embeddings eventually\"\n",
    "\n",
    "    if cf == GaussianKL:\n",
    "        exp_data = GaussianKL_results(dataset.name, htsne, ex_iterations, perp, num_p, pca,\n",
    "                       main_iterations, cf, hyp_var, size_tol, max_dist_H, max_dist, correct_grad,\n",
    "                       grad_scale_fix, exact, exaggeration_factor, optim_procedure, description)\n",
    "        \n",
    "    elif cf == HyperbolicKL:\n",
    "        exp_data = HyperbolicKL_results(dataset.name, htsne, ex_iterations, perp, num_p, pca,\n",
    "                       main_iterations, cf, correct_grad, grad_scale_fix, exact, \n",
    "                       exaggeration_factor, optim_procedure, description)\n",
    "\n",
    "    animation_step = 25\n",
    "    save_experiment_results(save_folder, None, emb_fig, opt_params, dataLabels, exp_data, hyperbolicEmbedding, htsne=htsne, step=animation_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Hyperbolic Variance\n",
    "\n",
    "First we compute how many Euclidean variances wide the maximum distance between 2 datapoints is:\n",
    "$$ c = \\frac{D_{max}}{\\sigma^2_E} $$\n",
    "\n",
    "By setting the Hyperbolic variance to (below) we gain a value for the Hyperbolic variance that hopefully causes the Hyperbolic embeddings to stay reasonably within the visualizable part of the Poincare Disk.\n",
    "$$ \\sigma^2_D = \\frac{d^H(-0.99, 0.99)}{c}$$\n",
    "\n",
    "[TODO: Expand on elaboration]\n",
    "This is because I suspect that by relating the variance (like in above), the embedding process will cause embeddings to not spread out too much. Since when we are matching the probability distributions, the probabilities will fall off quick as we get multiple variances away. By relating the variance to the maximum distance we want out of our embedding, and at the same time having a $$ c $$ that encodes the high dimensional relatonship between embedding distance size and variance, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store experiment results and data into a dedicated folde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical analysis of gradients\n",
    "\n",
    "Investigate the probability distributions of high/low dim. respectively between the 2 gradients\n",
    "Investigate ratio of gradients between correct/wrong gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperbolic distance between y1, y2\n",
    "def hyp_dist(y1, y2):\n",
    "    diff = y1 - y2\n",
    "    num = diff.dot(diff)                         # numerator\n",
    "    denum = (1 - y1.dot(y1)) * (1 - y2.dot(y2))  # denumerator\n",
    "    dist = np.arccosh(1 + 2 * (num / denum))\n",
    "\n",
    "    return dist\n",
    "\n",
    "\n",
    "# Compute the probability distribution associated with our embedding.\n",
    "# Low dimensional affinities, computed from embeddings \n",
    "def compute_affinities(embedding_data):\n",
    "    Q = np.zeros((embedding_data.shape[0], embedding_data.shape[0]))\n",
    "    for i in range(Q.shape[0]):\n",
    "        for j in range(i + 1, Q.shape[1]):\n",
    "            dist = hyp_dist(embedding_data[i], embedding_data[j])       # hyp. distance\n",
    "            dist = 1. / (1 + dist**2)                                   # t-distrib. \"distance\"\n",
    "            Q[i][j] = dist\n",
    "            Q[j][i] = dist\n",
    "\n",
    "    # Normalize distances to probabilities\n",
    "    # return Q / Q.sum()\n",
    "    return Q / Q.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recomputing probability matrices\n",
    "P = V.toarray()                         # high dim. affinity\n",
    "Q = compute_affinities(data[\"Emb\"])     # low dim. affinity\n",
    "\n",
    "print(P[0])\n",
    "print()\n",
    "print(Q[0])\n",
    "\n",
    "print(P - Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of behaviour at beginning (early iterations)\n",
    "\n",
    "1. hyp. distances are small $d^{H}_{ij}$ approaches 0 (since $cosh^{-1} goes to 0), since its argument goes to 1.\n",
    "\n",
    "2. This means t-distrib. probability goes to 1, $(1 + (d^{H}_{ij})^2)^{-1}$ goes to 1.\n",
    "So once normalized, all probabilities $q^{H}_{ij}$ are almost the same for all $ij$, so we basically have an uniform distribution for $q^{H}_{ij}$\n",
    "\n",
    "3. Why are the wrong gradients so much bigger?\n",
    "\n",
    "Correct Gradient expression:\n",
    "$$ 4  \\sum_{j} (p_{ij} - q^{H}_{ij}) (1 + (d^{H}_{ij})^2)^{-1}  d^{H}_{ij} \\frac{d^{H}_{ij}}{y_i}$$\n",
    "\n",
    "\\\n",
    "Wrong Gradient expresion:\n",
    "$$ 4  \\sum_{j} (p_{ij} - q^{H}_{ij}) (1 + (d^{H}_{ij})^2)^{-1} \\frac{d^{H}_{ij}}{y_i}$$\n",
    "\n",
    "\\\n",
    "Because in early iterations, $ d^{H}_{ij} $ is approximately 0, the correct gradient is very small, until points haved moved sufficiently for this term to increase in value.\n",
    "\n",
    "Can this possibly replace early exaggeration? Maybe.. \n",
    "At the beginning, points are updated in very small amounts. But this applies to both attractive and repulsive forces.\n",
    "\n",
    "At some points, points are \"distant\" enough, for $ d^{H}_{ij} $ to not matter much anymore. \n",
    "\n",
    "$ d^{H}_{ij} $ goes up a lot (to possibly infinity) for larger distances. This means that forces between distant points get heavily amplified. Is this an issue though? Since very distant forces should be more or less 0?\n",
    "\n",
    "4. Is the use of the t-distribution justified in hyperbolic space?\n",
    "Is it making repellant points feel a repellant force for way too far distances?\n",
    "What do p, q look like for points far away? Is the heavy-tailedness a problem in hyperbolic space?\n",
    "\n",
    "Despite large distances, $ q^{H}_{ij} $ is never able to reach the \"correct\" probabilities due to the use of the t-distribution? Since the heavy tails requires too big of distances for things to converge properly. Hence points keep getting pushed out? \n",
    "\n",
    "We can try using a regular gaussian distribution for $ q^{H}_{ij} $. Rederive the gradient, and experiment with that.\n",
    "But do we then need to take the symmetric version into account?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delft_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
