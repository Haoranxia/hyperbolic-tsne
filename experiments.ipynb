{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet Dataset experiment #\n",
    "What do I wish to achieve with this experiment(s)?\n",
    "\n",
    "### Correct vs Incorrect gradient behaviour expectation\n",
    "The correct gradient adds an additional factor to the attr./rep. forces. I expect this to speed up convergence. \n",
    "\n",
    "The attractive term (1st term) is a sparse term (most values in summation are 0), and thus I expect less of an effect on attraction (unless perpelxity is increased). \n",
    "\n",
    "The repulsive term (2nd term) will be more strongly affected. As points go apart $q^{H}_{ij}$ goes down (to 0) but, $d^{H}_{ij}$, the extra term will increase (exponentially?)\n",
    "\n",
    "So the expectation is that points will repel more strongly, points will be repelled faster and more strongly.\n",
    "\n",
    "\n",
    "### Early exaggeration\n",
    "At the beginning we scale up the high dim. prob.'s so the attractive term dominates more. This is so similar points already start clustering together before being separated by the repulsive force. \n",
    "\n",
    "\n",
    "\n",
    "### WordNet specific experiment animo\n",
    "1. WordNet is supposedly a hierarchical (hyperbolic) dataset\n",
    "    - Is this a property that we can \"obviously\" observe from the embeddings?\n",
    "    - We expect the WordNet visualizations to display obivous hyperbolic/hierarchical structure\n",
    "2. Is there a noteworthy difference in speed/embedding quality for the incorrect vs correct gradient?\n",
    "\n",
    "3. Enable acceleration\n",
    "\n",
    "\n",
    "### WordNet dataset experiment notes\n",
    "1. My current setup can't handle > 0.1 numpoints well. It takes a long time (So test stuff with less data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import traceback\n",
    "\n",
    "from hyperbolicTSNE import Datasets\n",
    "from hyperbolicTSNE.visualization import plot_poincare, animate\n",
    "from hyperbolicTSNE import load_data, Datasets, SequentialOptimizer, initialization, HyperbolicTSNE\n",
    "from hyperbolicTSNE.cost_functions_ import CoSNE, HyperbolicKL, GlobalHSNE\n",
    "from hyperbolicTSNE.util import find_last_embedding, opt_config, initialize_logger, write_data, store_visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_home = \"datasets\"\n",
    "log_path = \"temp/poincare/\"  # path for saving embedding snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# Different experiment values\n",
    "# num_points = [0.25, 0.5, -1]                     # -1 means all the data\n",
    "# perplexities = [10, 30, 50, 80, 100, 150]\n",
    "\n",
    "# Parameters specific to experiments\n",
    "# MNIST -> (70000, 784)\n",
    "# C_ELEGANS -> (89701, 20222)\n",
    "datasets = [Datasets.C_ELEGANS, Datasets.PLANARIA] \n",
    "num_points = [0.5]\n",
    "perplexities = [50]\n",
    "correct_gradients = [False, True]                # NOTE: Recompile with correct flag (GRAD_FIX flag)\n",
    "exact = False                                    # Exact computation or BH estimation of gradient\n",
    "pca_components = 50                              # Whether to use pca initialization of high dim. data or not\n",
    "grad_scale_fix = True                            # Whether we multiply the gradient by the inverse metric tensor of hyperbolic space or not\n",
    "                                                 # Note that the correct hyperoblic gradient has an inverse metric tensor factor\n",
    "learning_rates_factors = [1000, 10000]\n",
    "\n",
    "# General parameters to be set for all experiments\n",
    "exaggeration_factor = 12\n",
    "ex_iterations = 250\n",
    "main_iterations = 1000\n",
    "\n",
    "# Parameters to be set regarding saving of data\n",
    "# TODO: Automate cost identification for saving purposes. Right now its done manually\n",
    "data_header = ['dataset', 'data_size', \n",
    "                'data_dim', 'pca_init', \n",
    "                'perplexity', 'pca_components',\n",
    "                'cost_function_value', 'cost_function', \n",
    "                'runtime', 'total_iterations', \n",
    "                'exact', 'correct_gradient', 'grad_scale_fix', 'lr_factor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the cost function\n",
    "# cf = CoSNE\n",
    "# lambda_1 = 20\n",
    "# lambda_2 = 0.1\n",
    "\n",
    "# def CoSNE_params(x_norm, l1, l2, n_samples):\n",
    "#     return {\n",
    "#         \"x_norm\" : x_norm,\n",
    "#         \"lambda_1\" : l1,\n",
    "#         \"lambda_2\" : l2,\n",
    "#         \"n_samples\" : n_samples\n",
    "#     }\n",
    "\n",
    "# print(cf.class_str())\n",
    "# cf = GlobalHSNE\n",
    "# lbda = 0.1\n",
    "# def GlobalHSNE_params(P_hat, lbda, n_samples):\n",
    "#     return {\n",
    "#         \"P_hat\": P_hat,\n",
    "#         \"lbda\": lbda,\n",
    "#         \"n_samples\": n_samples,\n",
    "#     }\n",
    "cf = HyperbolicKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Code for WordNet Dataset experiments\n",
    "\"\"\"\n",
    "for dataset in datasets:\n",
    "    for correct_gradient in correct_gradients:\n",
    "        for lr in learning_rates_factors:\n",
    "            for np in num_points:\n",
    "                for perplexity in perplexities:\n",
    "                    ##################\n",
    "                    # INITIALIZATION #\n",
    "                    ##################\n",
    "                    print(\"\\nExperiment: \", dataset, \"num_points: \", np, \"perp: \", perplexity, \"correct gradient: \", correct_gradient)\n",
    "\n",
    "                    # Load the data\n",
    "                    # NOTE: if we don't use -1 data, we need another argument _ to catch the sample indices\n",
    "                    dataX, dataLabels, D, V, *rest = load_data(\n",
    "                        dataset, \n",
    "                        data_home=data_home, \n",
    "                        pca_components=pca_components,\n",
    "                        random_state=seed, \n",
    "                        to_return=\"X_labels_D_V\",\n",
    "                        hd_params={\"perplexity\": perplexity}, \n",
    "                        sample=np, \n",
    "                        knn_method=\"hnswlib\"  # we use an approximation of high-dimensional neighbors to speed up computations\n",
    "                    )\n",
    "                    \n",
    "                    # Compute initial embedding in Poincare disk (PCA embedding)\n",
    "                    X_embedded = initialization(\n",
    "                        n_samples=dataX.shape[0], \n",
    "                        n_components=2,\n",
    "                        X=dataX,\n",
    "                        random_state=seed,\n",
    "                        method=\"pca\"\n",
    "                    ) \n",
    "\n",
    "                    # Initialize config and parameters\n",
    "                    learning_rate = (dataX.shape[0] * 1) / (exaggeration_factor * lr)\n",
    "\n",
    "                    opt_conf = opt_config(cf, learning_rate, exaggeration_factor, ex_iterations, main_iterations, exact, correct_gradient)\n",
    "                    opt_params = SequentialOptimizer.sequence_poincare(**opt_conf) \n",
    "                    opt_params, opt_conf = initialize_logger(opt_params, opt_conf, log_path=log_path, grad_path=None)\n",
    "\n",
    "                    # TODO: In the case of CoSNE, we must pass additional parameters to the cost function\n",
    "                    # x_norm = (X_embedded * X_embedded).sum(axis=1)\n",
    "                    # opt_params[\"cf_params\"].update(CoSNE_params(x_norm, lambda_1, lambda_2, dataX.shape[0]))  # Add CoSNE params to cf_params dict\n",
    "                    # print(x_norm.shape, dataX.shape[0], X_embedded.shape)\n",
    "                    opt_params[\"cf_params\"].update({\"grad_fix\" : correct_gradient})     # So the cost function knows which gradient to use\n",
    "                    print(opt_params[\"cf_params\"])\n",
    "\n",
    "                    #########################\n",
    "                    # EMBEDDING COMPUTATION #\n",
    "                    #########################\n",
    "                    # Set up H-TSNE object \n",
    "                    htsne = HyperbolicTSNE(\n",
    "                        init=X_embedded, \n",
    "                        n_components=2, \n",
    "                        metric=\"precomputed\", \n",
    "                        verbose=True, \n",
    "                        opt_method=SequentialOptimizer,         # the optimizater we use\n",
    "                        opt_params=opt_params              # the parameters for the optimizers\n",
    "                    )\n",
    "\n",
    "                    # Compute embedding:\n",
    "                    try:\n",
    "                        hyperbolicEmbedding = htsne.fit_transform((D, V))\n",
    "                        \n",
    "                    except ValueError:\n",
    "                        hyperbolicEmbedding = find_last_embedding(log_path)\n",
    "                        traceback.print_exc()\n",
    "\n",
    "\n",
    "                    ###################\n",
    "                    # STORING RESULTS #\n",
    "                    ###################\n",
    "                    # Write results to csv\n",
    "                    data_row = [\n",
    "                        dataset, np, dataX.shape[1], pca_components,\n",
    "                        perplexity, pca_components, htsne.cf, cf.class_str(), htsne.runtime, htsne.its,\n",
    "                        opt_conf['exact'], correct_gradient, grad_scale_fix, lr\n",
    "                    ]\n",
    "                \n",
    "                    results_path = f\"results/csvs/{dataset.name}.csv\"       # Path of csv file to store results\n",
    "                    save_folder = f\"results/{dataset.name}\"                 # Path of folder to store visualizations\n",
    "                    \n",
    "                    # Name of visualization file (.png and .gif)\n",
    "                    file_name = f\"{save_folder}/exact:{exact}_correctgrad:{correct_gradient}_scalefix:{grad_scale_fix}_points:{np}_pca:{pca_components}_perp:{perplexity}_cf:{cf.class_str()}_lrfactor:{lr}\"\n",
    "\n",
    "                    # Store results\n",
    "                    write_data(data_header, data_row, file_path=results_path)\n",
    "                    store_visuals(hyperbolicEmbedding, dataLabels, save_folder, file_name, opt_params)\n",
    "                    \n",
    "\n",
    "\"\"\" \n",
    "Dimensions for wordnet data\n",
    "x_norm: (41058,) dataX.shape[0]: 41058 X_embedded: (41058, 2)\n",
    "grad1.shape: (82116,) Y.shape: (82116,) x_norm: (41058,)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delft_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
