{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future work\n",
    "\n",
    "1. Possibly look into h-sne and co-sne\n",
    "2. t-sne vs sne in Hyperbolic Space in different models\n",
    "3. Motivate t-sne to sne for Poincare Disk more (t-distrib. to gaussian)\n",
    "4. How to measure \"crowding\" in Hyperbolic Space. Does crowding happen with the Gaussian?\n",
    "   - Some qualitative evidence such as euclidean sne embedding, euclidean t-sne, hyperbolic sne \n",
    "     to gain insight into crowding\n",
    "   - Look into t-sne paper, provide some insight into whether crowding is an issue\n",
    "   - Look into SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General storyline\n",
    "\n",
    "1. **Incorrect vs correct gradient** \\\n",
    "   We go from the incorrect to the correct gradient. Why do we do this? Benefits? etc..\n",
    "\n",
    "2. **Correct gradient t-distribution problems** \\\n",
    "   Now that the use of the correct gradient is properly justified, what now? What problems do we get here?\n",
    "\n",
    "3. **Gaussian instead of t-distribution** \\\n",
    "   The problem is that the t-distribution is not fit for embeddings on the Poincare Disk (explain). The Gaussian is a better fit (explain).\n",
    "\n",
    "4. **Capturing hierarchical relationships** \\\n",
    "   We can drive the point accross further by embedding tree-like data (using both Gaussian & t-distrib.).\n",
    "   Here we notice that the Gaussian generally does a better job. (especially for \"nice\" trees)\n",
    "\n",
    "5. **Limitations** \\\n",
    "   This method starts to break down for data with large/long hierarchies, probably data with lots of clusters, many spread out clusters, etc.. \\\n",
    "   Because the visually significat part of the Poincare Disk can only embed so much, it is not fit for certain datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis structure related TODOs\n",
    "\n",
    "### Currently planned points & Experiments & TODOs\n",
    "The main points I want to get accross. What experiments fit under each point?\n",
    "\n",
    "1. **E1: Correct Gradient justification experiments** \\\n",
    "Firstly we compare hyperbolic t-sne correct vs. incorrect gradient:\n",
    "    1. Cost function convergence graphs for several datasets incl. tree data\n",
    "        - **Experiment**: Collect a bunch of correct vs. incorrect gradient experiments\n",
    "\n",
    "    2. Visual comparison of embeddings for several datasets: \n",
    "        - **Experiment**: Just take visualization results from (1.)\n",
    "\n",
    "    3. Cost function analysis: \n",
    "        - **Experiment**: From the graph of the cf values of the wrong gradient we can see that for a small nr. of iterations (used in Hunters' work), the embeddings haven't converged yet at all. The cf hasn't converged yet at all. This could indicate that the currently obtained embedding isn't final yet. The embedding has not \"correctly\" reproduced the original data yet. Therefore it hints at having to increase iterations until it converges before we can make any statements about it.\n",
    "\n",
    "    4. Visual comparison of the 2 gradient versions\n",
    "        - **Experiment**: (Discuss with supervisor). Visualizations of gradients but tbd.. how to do this properly\n",
    "\n",
    "2. **E2: Limitations of t-distrib. Gradient** \n",
    "    1. Show for (correct & wrong) gradient visualizations where embeddings are pushed out\n",
    "        - **Experiment**: Collect visualization results for all the datasets used in Hunters' paper. \n",
    "\n",
    "    2. Quality metrics of t-distrib. gradient \n",
    "        - **TODO**: Look into the different quality metrics\n",
    "\n",
    "    3. Show that in the limit Hunters' work (wrong gradient t-distrib.) also converges to a spread out embedding. This gives further credit to changing the distribution. \\\n",
    "        - **Experiment**: run \"example_basic_usage.ipynb\" in the original repository with the same parameters for a large amount of steps. Show that this leads to very distorted (spread out) embeddings too.\n",
    "\n",
    "3. **E3: Improvents by Gaussian Gradient**\n",
    "    1. Show (corrected) gaussian embeddings. Use same datasets & parameters as in t-distrib. section\n",
    "    2. Quality metrics of gaussian gradient\n",
    "\n",
    "4. **E4: Tree-data embeddings** \\\n",
    "Maybe this can be used to further highlight a limitation of the t-distrib. gradient. So we put this section between **E2** and **E3**? \n",
    "    1. Show tree-data embedding for t-distrib. and gaussian gradient. \\\n",
    "        Produce results for varying \"sized\" trees. (differing in depth. children etc...)\n",
    "    2. Quality metrics for this dataset\n",
    "\n",
    "5. **E5: Acceleration structure for Gaussian gradient (TODO)** \\\n",
    "Investigate acceleration on Gaussian gradient embeddings? (TBD...)\n",
    "    1. Proceed same as Hunter did in his thesis (TBD...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Currently collected correct vs incorrect gradient results:\n",
    "\n",
    "### MNIST\n",
    "1. laptop 107 (incorrect) vs pc 35 (correct):\\\n",
    "    perplexity=25, pca=50, num_points=0.05, exag=12, lr=2.916, exact=True\n",
    "\n",
    "2. laptop 108 (incorrect) vs pc 36 (correct):\\\n",
    "    perplexity=25, pca=50, num_points=0.05, exag=12, lr=29.16, exact=True\n",
    "\n",
    "### C_ELEGANS\n",
    "1. laptop 113 (incorrect) vs pc 40 (correct):\\\n",
    "    perplexity=30, pca=50, num_points=0.1, exag=12, lr=0.7475, exact=False\n",
    "\n",
    "2. laptop 114 (incorrect) vs pc 41 (correct):\\\n",
    "    perplexity=30, pca=50, num_points=0.1, exag=12, lr=7.475, exact=False\n",
    "\n",
    "3. laptop 115 (incorrect) vs pc 42 (correct):\\\n",
    "    perplexity=30, pca=50, num_points=0.1, exag=12, lr=74.75, exact=False\n",
    "\n",
    "### PLANARIA\n",
    "1. laptop 116 (incorrect) vs laptop 119 (correct):\\\n",
    "    perplexity=30, pca=50, num_points=0.1, exag=12, lr=0.18008, exact=False\n",
    "\n",
    "2. laptop 117 (incorrect) vs laptop 120 (correct):\\\n",
    "    perplexity=30, pca=50, num_points=0.1, exag=12, lr=18.008, exact=False\n",
    "\n",
    "3. laptop 118 (incorrect) vs laptop 121 (correct):\\\n",
    "perplexity=30, pca=50, num_points=0.1, exag=12, lr=18.008, exact=False\n",
    "\n",
    "### WORDNET\n",
    "\n",
    "### Tree-Data\n",
    "- Experiment 95, 96: Tree data embedding but embeddings did not converge\n",
    "- Experiment 97, 98: Tree data (simple tree) converged embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Currently collected Gaussian gradient experiments\n",
    "\n",
    "### MNIST\n",
    "- Experiment 71, 72: GaussianKL MNIST experiment results\n",
    "\n",
    "### C_ELEGANS\n",
    "- Experiment 88, 89: GaussianKL C_ELEGANS experiment results\n",
    "\n",
    "### PLANARIA\n",
    "- Experiments 122, 123, 124: GaussianKL PLANARIA experiment results\n",
    "\n",
    "### WORDNET\n",
    "\n",
    "### Tree-Data\n",
    "- Experiments 75 - 80 are more complicated tree experiments. Trees with larger depth, fewer children etc.. Results are less nice embeddings.\n",
    "\n",
    "- Experiments 83 - 87 are \"basic\" Tree data experiments. Fairly small and reasonable trees. Results are reasonable embeddings too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality retention metrics (TODO)\n",
    "- Scale-independent quality criteria for dimensionality reduction metric\n",
    "\n",
    "### Between correct & wrong\n",
    "- TBD\n",
    "\n",
    "### Between t-sne & Gaussian\n",
    "- Cost function comparisons metric (relative cf error -> see paper)\n",
    "\n",
    "- Visual comparisons\n",
    "\n",
    "- Neighbourhood retention metrics\n",
    "    1. 1-nearest neighbor error: \\\n",
    "    percentage of points whose nearest neighbor in embedding doesn't have the same class label as the query point (corresponding point in original dataset)\n",
    "\n",
    "    2. Precision/recall metric: \\\n",
    "    TODO: inquire about this\n",
    "    Select a random neighbourhood of various sizes (1, 2, 3, ..., k), compute the number of true positives, the nr. of points that are in high dim. and low dim.? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian gradient accel. results (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis structure\n",
    "\n",
    "1. Introduction\n",
    "    - **Introduce hyperbolic embeddings** \\\n",
    "    Firsly we briefly introduce hyperbolic embeddings and its use cases. Explain connection between hierarchical relationships, tree/graph/network embeddings and hyperbolic space. Mention acceleration structure by Hunter.\n",
    "\n",
    "    - **Explain my contributions, where the thesis is heading** \\\n",
    "    Secondly we talk about correcting the gradient:\\\n",
    "    We introduce a correction to the gradient(s) used in other work. Hint at methods/experiment section for proper justification but briefly state justifications here.\n",
    "    (Mathematical correctness, bring into question what the \"wrong gradient\" is optimizing, ie. not the described cf.) -> proper justification in methods/experiments\n",
    "    \\\n",
    "    Then we connect this to the original point of Hyperbolic embeddings (highlighting hierarchical structure). We explain how this is not (visually) captured by the t-distribution and therefore we propose using a Gaussian instead.\n",
    "    (Link back to original SNE, further justification in methods section)\n",
    "    Then we talk about the Gaussian Gradient as a follow-up contribution for hyperbolic embeddings and conclude that this seems to work well (further justified in methods/experiments). It seems that using a Gaussian we are able to succesfully capture latent hierarchical structure.\n",
    "\n",
    "    <!-- - **Why would people care** \\\n",
    "    Other versions used use a wrong gradient (possibly they didn't fully derive the gradient but just took euclidean t-sne gradient and substituted some terms). \\\n",
    "    Wrong gradient may lead to false insights as results do not optimize the correct cost function. Or rather, we don't know, can't fully explain what is being optimized. Link to experiments/methods where we empricially show (through analysis of cost function progression) that the 2 gradients act differently.\n",
    "    -->\n",
    "\n",
    "    - **Challenges** \n",
    "    TBD...\n",
    "\n",
    "\n",
    "2. Related work (Straightforward)\n",
    "    - data visualization (tsne)\n",
    "    - Hyperbolic space & Poincare disk\n",
    "    - Hunters' work, other hyperbolic visualization\n",
    "    - BH acceleration (maybe?)\n",
    "\n",
    "\n",
    "3. Background (Straightforward)\n",
    "    - Technical details about what is required to understand my thesis\n",
    "    - Hyperbolic space, t-sne/neighbour embedding methods, gradient descent\n",
    "\n",
    "\n",
    "4. Methods \\\n",
    "This section contains an overview of what I am contributing. Contains detailed reasons and motivations for what Im contributing (although empirical evidence is in the Experiments/Discussion sections).\n",
    "    - **Correct Gradient** \\\n",
    "        Justify the change to the correct gradient as with the following:\n",
    "        - Mathematical correctness argument, and bring into question what were even optimizing with the (wrong) gradient.\n",
    "        - A correct mathematical derivation of the gradient (appendix?)\n",
    "        - Empirical evidence of cost function convergence (to be elaborated on in Experiments section), to show that we are indeed not optimizing what we expect to be optimizing with the wrong gradient. (cost function)\n",
    "    \n",
    "    - **Tree-like data embedding** \\\n",
    "        Hyperbolic Space is supposed to embed trees (and graph/network-like) data well. (many references) \\\n",
    "        One contribution is an explicit experiment to test how trees are embedded using artificial tree-like data.\n",
    "        - Explain this idea with a some detail. Why do we care about embedding a tree. \n",
    "        - Emphasize how we use tree-like data as further evidence for the correct gradient, and t-sne/gaussian gradient contributions.\n",
    "        - Briefly go over how this data gets generated?\n",
    "        - Show some visualizations \n",
    "        - Use quantifiable quality metrics (in experiments & discussion)\n",
    "\n",
    "    - **Discuss limitations of t-sne distribution** \\\n",
    "        Justify this contribution step as follows:\n",
    "        - Show how both the correct (and incorrect, aka original gradient used) gradient push points towards the boundary. \\\n",
    "        Originally, the incorrect gradient seems to not do this, but if you keep the algorithm running, it will also converge to a similarily pushed out embedding.\n",
    "        - Explain why this is not desirable.\n",
    "        - Explain where this comes from (heavy tails of t-distribution, repulsive forces being too strong, the nature and limitations of using Poincare Disk for visualization)\n",
    "        - Visualization results & evaluation metric to further justify (in experiments & discussion)\n",
    "\n",
    "    - **Proposal of Gaussian distribution usage** \\\n",
    "        Motivate gaussian distribution usage: \n",
    "        - Origina SNE paper used gaussian\n",
    "        - t-distrib. was introduced for the crowding problem. Explain how in Hyperbolic space, this might be less of an issue (area/volume scales exponentially instead of linearly in 1D /quadratically in 2D)\n",
    "        - Gaussian has flatter tails, so forces aren't felt a smuch. This can be demonstrated mathematically.\n",
    "        - Visualization results & evaluation metric to further justify (in experiments & discussion)\n",
    "\n",
    "    - **Integrating everything with Hunter's work** \\\n",
    "        - Link everything back to the acceleration structure build by Hunter.\n",
    "        - Show that acceleration works equally as valid for Gaussian gradient.\n",
    "        - Maybe more??\n",
    "        \n",
    "\n",
    "5. Experiments \\\n",
    "    Talk about experiments that motivate this thesis, connect them to my contributions/goals. Experiment starts with a question targetting a contribution/goal, answer question using the experiment, relate it to the next experiment\n",
    "\n",
    "    - **E1: Correct Gradient justification experiments** \\\n",
    "    Firstly we compare hyperbolic t-sne correct vs. incorrect gradient:\n",
    "        1. Cost function convergence graphs for several datasets incl. tree data\n",
    "        2. Visual comparison of embeddings for several datasets \n",
    "        3. Visual comparison of the 2 gradient versions\n",
    "\n",
    "    - **E2: Limitations of t-distrib. Gradient** \n",
    "        1. Show for (correct & wrong) gradient visualizations where embeddings are pushed out\n",
    "        2. Quality metrics of t-distrib. gradient \n",
    "    \n",
    "    - **E3: Improvents by Gaussian Gradient**\n",
    "        1. Show (corrected) gaussian embeddings. Use same datasets as in t-distrib. section\n",
    "        2. Quality metrics of gaussian gradient\n",
    "\n",
    "    - **E4: Tree-data embeddings** \\\n",
    "    Maybe this can be used to further highlight a limitation of the t-distrib. gradient. So we put this section between **E2** and **E3**. \n",
    "        1. Show tree-data embedding for t-distrib. and gaussian gradient. \\\n",
    "           Produce results for varying \"sized\" trees. (differing in depth. children etc...)\n",
    "        2. Quality metrics for this dataset\n",
    "    \n",
    "    - **E5: Acceleration structure for Gaussian gradient** \\\n",
    "    Investigate acceleration on Gaussian gradient embeddings? (TBD...)\n",
    "        1. Proceed same as Hunter did in his thesis (TBD...)\n",
    "\n",
    "\n",
    "6. Discussion\n",
    "    - Discuss experiments and connect them together to shape a strong motivation for my goal\n",
    "    - Focussed on content of thesis (methods/experiments)\n",
    "\n",
    "    - **Use E1 to justify Correct Gradient** \\\n",
    "    Here we elaborate and fully justify the use of the correct gradient over the wrong one using **E1**\n",
    "    We use the cost function graphs to highlight the problem with the wrong gradient (is not properly minimizing the objective in question), and show that the correct gradient does indeed minimize what we want. \n",
    "\n",
    "    - **E2 and t-distrib. limitations** \\\n",
    "    Here we use **E2** to convince the reader of the limitations of the t-distrib. gradient. We set the stage for the Gaussian gradient contribution. \n",
    "\n",
    "    - **E4: Tree-data justification** (maybe not as separate section) \\\n",
    "    Use tree-data embeddings to further justify Gaussian gradient over t-distrib. gradient. We can again use visuals, cost function comparisons, and quality retention metrics.\n",
    "    We can also justify this with related works that talk about tree/graph/network data embedding in Hyperbolic space.\n",
    "\n",
    "    - **E3: Gaussian gradient justification** \\\n",
    "    Now we show that the Gaussian gradient performs better in both visualization (and I assume also quality retention metrics).\n",
    "    We compare visuals, cost function experiment results, quality retention metrics to justify the usage of the Gaussian gradient. \n",
    "    \n",
    "\n",
    "7. Conclusion (TBD)\n",
    "    - Link experiments, discussion back to the original questions, goals, theme of the thesis. \n",
    "    - Present recommendations, risks, guidelines based on experiments/discussion\n",
    "    - Put things in context globally"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delft_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
