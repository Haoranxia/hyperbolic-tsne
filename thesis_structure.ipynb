{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis structure related TODOs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis structure\n",
    "\n",
    "1. Introduction\n",
    "    - **Introduce hyperbolic embeddings**\n",
    "    Firsly we briefly introduce hyperbolic embeddings and its use cases. Explain connection between hierarchical relationships, tree/graph/network embeddings and hyperbolic space. Mention acceleration structure by Hunter.\n",
    "\n",
    "    - **Explain my contributions, where the thesis is heading** \\\n",
    "    Secondly we talk about correcting the gradient:\\\n",
    "    We introduce a correction to the gradient(s) used in other work. Hint at methods/experiment section for proper justification but briefly state justifications here.\n",
    "    \\\n",
    "    Then we talk about the Gaussian Gradient as a follow-up contribution for hyperbolic embeddings. Briefly explain why this contribution is significant.\n",
    "\n",
    "    <!-- - **Why would people care** \\\n",
    "    Other versions used use a wrong gradient (possibly they didn't fully derive the gradient but just took euclidean t-sne gradient and substituted some terms). \\\n",
    "    Wrong gradient may lead to false insights as results do not optimize the correct cost function. Or rather, we don't know, can't fully explain what is being optimized. Link to experiments/methods where we empricially show (through analysis of cost function progression) that the 2 gradients act differently.\n",
    "    -->\n",
    "\n",
    "    - **Challenges** \n",
    "    TBD...\n",
    "\n",
    "\n",
    "2. Related work (Straightforward)\n",
    "    - data visualization (tsne)\n",
    "    - Hyperbolic space & Visualizations\n",
    "    - Hunters' work, other hyperbolic visualization\n",
    "    - BH acceleration\n",
    "\n",
    "\n",
    "3. Background (Straightforward)\n",
    "    - Technical details about what is required to understand my thesis\n",
    "    - Hyperbolic space, t-sne/neighbour embedding methods, gradient descent\n",
    "\n",
    "\n",
    "4. Methods \\\n",
    "This section contains an overview of what I am contributing. Contains detailed reasons and motivations for what Im contributing (although empirical evidence is in the Experiments/Discussion sections).\n",
    "    - **Correct Gradient** \\\n",
    "        Justify the change to the correct gradient as with the following:\n",
    "        - Mathematical correctness argument, and bring into question what were even optimizing with the (wrong) gradient.\n",
    "        - A correct mathematical derivation of the gradient (appendix?)\n",
    "        - Empirical evidence of cost function convergence (to be elaborated on in Experiments section), to show that we are indeed not optimizing what we expect to be optimizing with the wrong gradient. (cost function)\n",
    "    \n",
    "    - **Tree-like data embedding** \\\n",
    "        Hyperbolic Space is supposed to embed trees (and graph/network-like) data well. (many references) \\\n",
    "        One contribution is an explicit experiment to test how trees are embedded using artificial tree-like data.\n",
    "        - Explain this idea with a some detail. Why do we care about embedding a tree. \n",
    "        - Emphasize how we use tree-like data as further evidence for the correct gradient, and t-sne/gaussian gradient contributions.\n",
    "        - Briefly go over how this data gets generated?\n",
    "        - Show some visualizations \n",
    "        - Use quantifiable quality metrics (in experiments & discussion)\n",
    "\n",
    "    - **Discuss limitations of t-sne distribution** \\\n",
    "        Justify this contribution step as follows:\n",
    "        - Show how both the correct (and incorrect, aka original gradient used) gradient push points towards the boundary. \\\n",
    "        Originally, the incorrect gradient seems to not do this, but if you keep the algorithm running, it will also converge to a similarily pushed out embedding.\n",
    "        - Explain why this is not desirable.\n",
    "        - Explain where this comes from (heavy tails of t-distribution, repulsive forces being too strong, the nature and limitations of using Poincare Disk for visualization)\n",
    "        - Visualization results & evaluation metric to further justify (in experiments & discussion)\n",
    "\n",
    "    - **Proposal of Gaussian distribution usage** \\\n",
    "        Motivate gaussian distribution usage: \n",
    "        - Origina SNE paper used gaussian\n",
    "        - t-distrib. was introduced for the crowding problem. Explain how in Hyperbolic space, this might be less of an issue (area/volume scales exponentially instead of linearly in 1D /quadratically in 2D)\n",
    "        - Gaussian has flatter tails, so forces aren't felt a smuch. This can be demonstrated mathematically.\n",
    "        - Visualization results & evaluation metric to further justify (in experiments & discussion)\n",
    "\n",
    "    - **Integrating everything with Hunter's work** \\\n",
    "        - Link everything back to the acceleration structure build by Hunter.\n",
    "        - Show that acceleration works equally as valid for Gaussian gradient.\n",
    "        - Maybe more??\n",
    "        \n",
    "\n",
    "5. Experiments \\\n",
    "    Talk about experiments that motivate this thesis, connect them to my contributions/goals. Experiment starts with a question targetting a contribution/goal, answer question using the experiment, relate it to the next experiment\n",
    "\n",
    "    - **E1: Correct Gradient justification experiments** \\\n",
    "    Firstly we compare hyperbolic t-sne correct vs. incorrect gradient:\n",
    "        1. Cost function convergence graphs for several datasets incl. tree data\n",
    "        2. Visual comparison of embeddings for several datasets \n",
    "        3. Visual comparison of the 2 gradient versions\n",
    "\n",
    "    - **E2: Limitations of t-distrib. Gradient** \n",
    "        1. Show for (correct & wrong) gradient visualizations where embeddings are pushed out\n",
    "        2. Quality metrics of t-distrib. gradient \n",
    "    \n",
    "    - **E3: Improvents by Gaussian Gradient**\n",
    "        1. Show (corrected) gaussian embeddings. Use same datasets as in t-distrib. section\n",
    "        2. Quality metrics of gaussian gradient\n",
    "\n",
    "    - **E4: Tree-data embeddings** \\\n",
    "    Maybe this can be used to further highlight a limitation of the t-distrib. gradient. So we put this section between **E2** and **E3**. \n",
    "        1. Show tree-data embedding for t-distrib. and gaussian gradient. \\\n",
    "           Produce results for varying \"sized\" trees. (differing in depth. children etc...)\n",
    "        2. Quality metrics for this dataset\n",
    "    \n",
    "    - **E5: Acceleration structure for Gaussian gradient** \\\n",
    "    Investigate acceleration on Gaussian gradient embeddings? (TBD...)\n",
    "        1. Proceed same as Hunter did in his thesis (TBD...)\n",
    "\n",
    "\n",
    "6. Discussion\n",
    "    - Discuss experiments and connect them together to shape a strong motivation for my goal\n",
    "    - Focussed on content of thesis (methods/experiments)\n",
    "\n",
    "    - **Use E1 to justify Correct Gradient** \\\n",
    "    Here we elaborate and fully justify the use of the correct gradient over the wrong one using **E1**\n",
    "    We use the cost function graphs to highlight the problem with the wrong gradient (is not properly minimizing the objective in question), and show that the correct gradient does indeed minimize what we want. \n",
    "\n",
    "    - **E2 and t-distrib. limitations** \\\n",
    "    Here we use **E2** to convince the reader of the limitations of the t-distrib. gradient. We set the stage for the Gaussian gradient contribution. \n",
    "\n",
    "    - **E4: Tree-data justification** (maybe not as separate section) \\\n",
    "    Use tree-data embeddings to further justify Gaussian gradient over t-distrib. gradient. We can again use visuals, cost function comparisons, and quality retention metrics.\n",
    "    We can also justify this with related works that talk about tree/graph/network data embedding in Hyperbolic space.\n",
    "\n",
    "    - **E3: Gaussian gradient justification** \\\n",
    "    Now we show that the Gaussian gradient performs better in both visualization (and I assume also quality retention metrics).\n",
    "    We compare visuals, cost function experiment results, quality retention metrics to justify the usage of the Gaussian gradient. \n",
    "    \n",
    "\n",
    "7. Conclusion (TBD)\n",
    "    - Link experiments, discussion back to the original questions, goals, theme of the thesis. \n",
    "    - Present recommendations, risks, guidelines based on experiments/discussion\n",
    "    - Put things in context globally"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delft_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
