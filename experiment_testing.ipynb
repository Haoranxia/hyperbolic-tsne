{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import traceback\n",
    "import numpy as np\n",
    "\n",
    "from hyperbolicTSNE import Datasets, load_data\n",
    "from hyperbolicTSNE import Datasets\n",
    "from hyperbolicTSNE.visualization import plot_poincare, animate\n",
    "\n",
    "from data_gen import find_children, find_depth, generate_Tree_D_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          4.28194202 11.94998164 15.1694803  16.25150787 18.0565542 ]\n",
      " [ 4.28194202  0.         12.9437631  16.24175564 16.6771032  17.27038277]\n",
      " [11.94998164 12.9437631   0.          2.07859554 28.20148951 33.2260345 ]\n",
      " [15.1694803  16.24175564  2.07859554  0.         29.6208663  33.51213841]\n",
      " [16.25150787 16.6771032  28.20148951 29.6208663   0.          4.10485562]\n",
      " [18.0565542  17.27038277 33.2260345  33.51213841  4.10485562  0.        ]]\n",
      "True\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Main parameters\n",
    "dist = 10\n",
    "depth = 2\n",
    "n_children = 2\n",
    "cluster_size = 2\n",
    "sigma = 2\n",
    "mu = 5\n",
    "n_nodes = sum(np.power(n_children, d) for d in range(depth))\n",
    "\n",
    "# Initial distance matrix D\n",
    "D, V = generate_Tree_D_V(cluster_size, n_children, n_nodes, mu, sigma, dist)\n",
    "print(D)\n",
    "print(np.allclose(D, D.T))\n",
    "\n",
    "print(V.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "(3, 3)\n",
      "(3, 3)\n",
      "(3, 3)\n",
      "(3, 3)\n",
      "(3, 4, 3)\n",
      "[[[0 1 2]\n",
      "  [0 1 2]\n",
      "  [0 1 2]\n",
      "  [0 1 2]]\n",
      "\n",
      " [[3 4 5]\n",
      "  [3 4 5]\n",
      "  [3 4 5]\n",
      "  [3 4 5]]\n",
      "\n",
      " [[6 7 8]\n",
      "  [6 7 8]\n",
      "  [6 7 8]\n",
      "  [6 7 8]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original (n, n) array\n",
    "n = 3\n",
    "d = 4\n",
    "arr = np.arange(n * n).reshape(n, n)\n",
    "\n",
    "print(arr.shape)\n",
    "for arr in [arr] * d:\n",
    "    print(arr.shape)\n",
    "\n",
    "\n",
    "# Stack the (n, n) array d times along a new axis (axis 1 for stacking vertically)\n",
    "stacked_arr = np.stack([arr] * d, axis=1)\n",
    "\n",
    "print(stacked_arr.shape)  # Output: (3, 4, 3)\n",
    "print(stacked_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-data testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "from __future__ import annotations \n",
    "\n",
    "class TreeDataset():\n",
    "    def __init__(self, n_data, n_children, depth):\n",
    "        # Basic parameters\n",
    "        self.n_data = n_data            # nr. of datapoints per node\n",
    "        self.n_children = n_children    # nr. of children per node \n",
    "        self.depth = depth              # depth of the tree\n",
    "        self.dist = 20\n",
    "\n",
    "        # Calculate size of D, V\n",
    "        self.n_nodes = np.sum([np.power(n_children, p) for p in range(depth + 1)])\n",
    "        size = n_data * self.n_nodes\n",
    "\n",
    "        # Build the tree\n",
    "        self.tree = TreeNode(self.n_data, None, n_children, depth, self.dist, self.n_nodes)\n",
    "\n",
    "\n",
    "        # D[n_data*i : n_data*(i+1)] \n",
    "        # Corresponds to the distance rows of node i\n",
    "        #\n",
    "        # D[n_data*i : n_data*(i+1)][n_data*j : n_data*(j+1)] \n",
    "        # Corresponds to distance blocks of node i with node j\n",
    "        self.D = np.zeros((size, size))\n",
    "        self.V = np.zeros((size, size))\n",
    "\n",
    "\n",
    "    def compute_distances(self, tree_node:TreeNode):\n",
    "        \"\"\"\n",
    "        For each node cluster in our tree, compute a list of distances to other node clusters\n",
    "        \"\"\" \n",
    "        if tree_node.children is None:\n",
    "            return \n",
    "        \n",
    "        # Compute distances of current node to all other nodes\n",
    "        tree_node.compute_distances(self.n_nodes)\n",
    "\n",
    "        # Perform same computation in children\n",
    "        for child in tree_node.children:\n",
    "            pass \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_hierarchical_D_V(cluster_size, n_children, depth):\n",
    "    \"\"\"\n",
    "    Generates a D (distance) and V (affinity) matrix for a tree-like/hierarchical dataset\n",
    "    where we have 'n_children' children per node, with a depth of 'depth'\n",
    "\n",
    "    depth = 0 means only a root, etc..\n",
    "    \"\"\"\n",
    "\n",
    "    n_nodes = np.sum([np.power(n_children, d) for d in range(depth + 1)])\n",
    "    sigma = 4       # parameters for distance sampling inside of a cluster\n",
    "    mu = 4          \n",
    "    dist = 50       # distance to other clusters\n",
    "    \n",
    "    # D[i, n, j, m] = src_node i; src_node cluster point n; tgt_node j, tgt_node cluster m\n",
    "    D = np.zeros((n_nodes, cluster_size, n_nodes, cluster_size))\n",
    "\n",
    "    # Initialize diagonal entries (node i=a with j=a), where distances between different\n",
    "    # points in the same cluster are randomly drawn\n",
    "    for node in n_nodes:\n",
    "        dists = np.abs(sigma * np.random.randn(cluster_size, cluster_size) + mu)\n",
    "        D[node, :, node, :] = np.triu(dists, 1)\n",
    "        D[node, :, node, :] = D[node, :, node, :] + D[node, :, node, :].T\n",
    "        np.fill_diagonal(D[node, :, node, :], 0)\n",
    "\n",
    "\n",
    "    # Compute distances to all children for every node\n",
    "    for node in n_nodes:\n",
    "        compute_child_distances(D, node, n_children, dist, n_nodes)\n",
    "\n",
    "    return D, V\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_child_distances(D, root, n, dist, n_nodes):\n",
    "    \"\"\" \n",
    "    D:      Distance matrix\n",
    "    root:   Index of root node we wish to compute for\n",
    "    n:      Nr. of direct children per node\n",
    "    dist:   Distance between each node cluster\n",
    "    \"\"\"\n",
    "    # Assume dist distance between the cluster means\n",
    "    # so distance between individual points would be: d(p_1, mean_1) + dist + d(p_2, mean_2)\n",
    "    child_idxs = [n * root + i for i in range(1, n + 1)]\n",
    "\n",
    "    # Base case, no more children, end of tree\n",
    "    if root >= n_nodes:\n",
    "        return\n",
    "\n",
    "    # Loop over children, recurse down and compute child distances bottom up\n",
    "    for child in child_idxs:\n",
    "        # Can't go out of bounds\n",
    "        if  child >= n_nodes:\n",
    "            return \n",
    "        \n",
    "        # Compute distances for each children\n",
    "        compute_child_distances(D, child, n, dist, n_nodes)\n",
    "\n",
    "        # Distance of this node to children (and childrens' children) is equal to:\n",
    "        # d(this nodes cluster, mean) + dist + D[root, :, child_idxs, :]\n",
    "        # Distance root node to its mean\n",
    "        root_mean_dist = D[root, :, root, :] - (D[root, :, root, :].sum() / (n_nodes * n_nodes))\n",
    "\n",
    "        # Distance child node to its mean\n",
    "        child_mean_dist = D[child , :, child , :] - (D[child, :, child, :].sum() / (n_nodes * n_nodes))\n",
    "\n",
    "        # Distance from root to child (idx)\n",
    "        D[root, :, child, :] = root_mean_dist + dist + child_mean_dist\n",
    "\n",
    "        # Distance from root to child's children?\n",
    "\n",
    "\n",
    "class TreeNode():\n",
    "    \"\"\" \n",
    "    Each tree node represents a cluster of data with a parent and children\n",
    "    Tree created breath-first. This means the following:\n",
    "\n",
    "    children_id's := (n_children * node_id + 1), ... (n_children * node_id + n_children)\n",
    "    parent_id     := floor(node_id / n_children) \n",
    "                  := floor([node_id + 1] / n_children) -- if node_id is a multiple of n_children\n",
    "    \"\"\"\n",
    "    def __init__(self, parent:TreeNode, depth:int, n_children:int, dist:float, node_ids: list):\n",
    "        self.parent: TreeNode   = parent            # Singular parent\n",
    "        self.depth: int         = depth    \n",
    "        self.dist: int          = dist\n",
    "        self.id: int            = node_ids.pop()\n",
    "        self.n_children: int    = n_children\n",
    "        self.children:list      = None\n",
    "\n",
    "    def update_distances(self, D):\n",
    "        pass\n",
    "\n",
    "    def create_children(self):\n",
    "        # Keep creating children until depth == 0\n",
    "        if self.depth > 0:\n",
    "            self.children: list = [TreeNode(self, self.depth - 1, self.n_children, self.dist, self.node_ids) for _ in range(self.n_children)]    \n",
    "\n",
    "\n",
    "def construct_tree_D_V(n_children, depth, dist):\n",
    "    n_nodes: int   = np.sum([np.power(n_children, d) for d in range(depth + 1)])    # total nr. of tree nodes\n",
    "    node_ids: list = [id for id in range(n_nodes, -1, -1)]                          # list of node id's\n",
    "    tree: TreeNode = TreeNode(None, depth, n_children, dist, node_ids)      \n",
    "\n",
    "    # Distance matrices    \n",
    "    D = np.zeros((n_nodes, n_nodes))\n",
    "    V = None \n",
    "\n",
    "    init_D(D, None, tree, dist)\n",
    "\n",
    "    return D, V\n",
    "\n",
    "\n",
    "def init_D(D: np.array, parent: TreeNode, current: TreeNode, dist: int):\n",
    "    if current.children is None:\n",
    "        D[parent.id, current.id] = dist\n",
    "        D[current.id, parent.id] = dist\n",
    "        return\n",
    "\n",
    "    for ch in current.children:\n",
    "        if parent is not None:\n",
    "            D[parent.id, current.id] = dist\n",
    "            D[current.id, parent.id] = dist\n",
    "\n",
    "        D[current.id, ch.id] = dist\n",
    "        D[ch.id, current.id] = dist\n",
    "\n",
    "        init_D(D, current, ch, dist)\n",
    "\n",
    "# n_children = 2\n",
    "# depth = 2\n",
    "# dist = 1\n",
    "\n",
    "# D, V = construct_tree_D_V(n_children, depth, dist)\n",
    "# print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing numpy functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "[[1 1 1]\n",
      " [4 4 4]\n",
      " [9 9 9]]\n",
      "[ 3 12 27]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA \n",
    "\n",
    "y = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n",
    "print(y.shape)\n",
    "\n",
    "yy = (y * y)\n",
    "print(yy)\n",
    "\n",
    "norms = yy.sum(axis=1)\n",
    "print(norms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing global hsne high dim. matrix function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperbolicTSNE.hd_mat_ import globalhsne_D_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haoranxia/Thesis-Delft/hyperbolic-tsne/hyperbolicTSNE/data_loaders.py:253: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(str(Path.joinpath(full_path, \"nouns.bin.best\")))\n"
     ]
    }
   ],
   "source": [
    "data_home = \"datasets\"\n",
    "log_path = \"temp/poincare/\"  # path for saving embedding snapshots\n",
    "\n",
    "dataset = Datasets.WORDNET\n",
    "\n",
    "pca_components = 50 \n",
    "seed = 42 \n",
    "perplexity = 50\n",
    "np = -1\n",
    "\n",
    "# load data\n",
    "dataX, dataLabels, D, V, *rest = load_data(\n",
    "        dataset, \n",
    "        data_home=data_home, \n",
    "        pca_components=pca_components,\n",
    "        random_state=seed, \n",
    "        to_return=\"X_labels_D_V\",\n",
    "        hd_params={\"perplexity\": perplexity}, \n",
    "        sample=np, \n",
    "        knn_method=\"hnswlib\"  # we use an approximation of high-dimensional neighbors to speed up computations\n",
    "    )\n",
    "\n",
    "# Regular p_ij found in D, V\n",
    "\n",
    "# Determine a good value for nr of neighbours\n",
    "# TODO: A better way to figure this out? Maybe use quadtree to compute D_hat, V_hat\n",
    "# n_neighbours = 100\n",
    "\n",
    "# # get global p_ij (p_ij hat in paper)\n",
    "# D_hat, V_hat = globalhsne_D_V(dataX, n_neighbours=n_neighbours)\n",
    "\n",
    "# print(D_hat.shape)\n",
    "# print(V_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82115, 11) (82115,)\n"
     ]
    }
   ],
   "source": [
    "print(dataX.shape, dataLabels.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delft_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
