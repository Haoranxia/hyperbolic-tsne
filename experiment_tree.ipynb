{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "from pathlib import Path \n",
    "import traceback\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperbolicTSNE imports\n",
    "from hyperbolicTSNE import Datasets, load_data\n",
    "from hyperbolicTSNE import Datasets, SequentialOptimizer, initialization, HyperbolicTSNE\n",
    "from hyperbolicTSNE.cost_functions_ import HyperbolicKL, GaussianKL\n",
    "from hyperbolicTSNE.util import find_last_embedding, opt_config, initialize_logger, save_experiment_results,  next_experiment_folder_id\n",
    "from hyperbolicTSNE.data_loaders import load_mnist\n",
    "from hyperbolicTSNE.hd_mat_ import hd_matrix\n",
    "from hyperbolicTSNE.visualization import plot_poincare, plot_tree\n",
    "\n",
    "# Data generation function\n",
    "from data_gen import generate_Tree_D, generate_Tree_V\n",
    "\n",
    "# Storing results functions\n",
    "from hyperbolicTSNE.util import GaussianKL_Tree_results, HyperbolicKL_Tree_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Hierarchical D, V \n",
    "\n",
    "Generate distance D, affinity V matrices based on a tree-like hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation parameters\n",
    "cluster_size = 10\n",
    "dist = 5\n",
    "n_children = 4\n",
    "depth = 3\n",
    "n_nodes = sum(np.power(n_children, d) for d in range(depth + 1))\n",
    "\n",
    "# Sample parameters\n",
    "mu = 0\n",
    "# sigma = np.sqrt(dist / n_children)       # std sigma\n",
    "sigma = np.sqrt(dist)\n",
    "\n",
    "# D, V matrix\n",
    "D = generate_Tree_D(cluster_size, n_children, n_nodes, mu, sigma, dist)\n",
    "V = generate_Tree_V(D, mean=mu, var=np.square(sigma))                       # Also known as P_ij Matrix\n",
    "\n",
    "# Datalabels := [label node 0, label node 1, ... label node n_nodes]\n",
    "dataLabels = np.array([[label for _ in range(cluster_size)] for label in range(n_nodes)]).flatten()\n",
    "print(\"dataLabels shape: \", dataLabels.shape)\n",
    "\n",
    "# print(V[0])\n",
    "# print(D[0])\n",
    "\n",
    "# Sanity check\n",
    "print(\"is D symmetric? \", np.allclose(D, D.T))\n",
    "print(\"is V symmetric? \", np.allclose(V, V.T))\n",
    "print(\"D shape: \", D.shape)\n",
    "\n",
    "# Turn V into a csr matrix (so it works with hyperbolicKL cf)\n",
    "V = csr_matrix(V)\n",
    "\n",
    "print(f\"V[0] max: {np.max(V[0])}, V[0] min: {np.min(V[0])}\")\n",
    "print(V[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = \"temp/poincare/\"  # path for saving embedding snapshots\n",
    "grad_path = \"temp/grad/\"     # NOTE: We will manually calculate the gradients\n",
    "data_home = \"datasets\"\n",
    "\n",
    "experiments_folder = \"./experiment_results/\"\n",
    "exp_id = next_experiment_folder_id(experiments_folder)\n",
    "\n",
    "seed = 42\n",
    "correct_gradient = [True, False]                   # NOTE: Recompile with correct flag (GRAD_FIX flag)\n",
    "exact = [True]                               # NOTE: Exact computation or BH estimation of gradient\n",
    "grad_scale_fix = True                        # Whether we multiply the gradient by the inverse metric tensor of hyperbolic space or not\n",
    "                                             # Note that the correct hyperoblic gradient has an inverse metric tensor factor\n",
    "cfs = [HyperbolicKL]\n",
    "\n",
    "# Simple experiment with no exaggeration\n",
    "exaggeration_factor = 12\n",
    "ex_iterations = 1000\n",
    "main_iterations = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute hyperbolic variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_dist(a, b):\n",
    "        num = (a-b) * (a-b)\n",
    "        denum = (1 - a*a) * (1 - b*b)\n",
    "        return np.arccosh(1 + 2 * (num / denum))\n",
    "\n",
    "# Computing hyperbolic variance using a heuristic method\n",
    "max_dist = np.max(D)                                    # max distance between 2 datapoints in high dim. space\n",
    "size_tol = 0.999\n",
    "max_dist_H = hyp_dist(-size_tol, size_tol)              # the max width we want to adhere to in hyperbolic embeddings\n",
    "hyp_sigma = (max_dist_H / max_dist) * sigma             # sigma computed in above cell\n",
    "hyp_var = np.square(hyp_sigma)\n",
    "\n",
    "print(f\"scaling sigma first - hyp sigma:{hyp_sigma}, hyp var:{hyp_var}\")\n",
    "\n",
    "# Second method for computing hyperoblic variance\n",
    "# hyp_sigma = (1. / max_dist) * sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hyp_dist(-0.99, 0.99))\n",
    "print(hyp_dist(-0.999, 0.999))\n",
    "print(hyp_dist(-0.9999, 0.9999))\n",
    "print(hyp_dist(-0.99999, -0.9999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "experiment_details = list(product(cfs, correct_gradient, exact))\n",
    "print(\"nr. of experiments: \", len(experiment_details))\n",
    "\n",
    "for exp in experiment_details:\n",
    "    print(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments\n",
    "for (cf, correct_grad, exact_grad) in experiment_details:\n",
    "    print(f\"[Experiment: {exp_id}] \\t cf: {cf.class_str()}, correct grad: {correct_grad},  exact grad: {exact_grad}\")\n",
    "    \n",
    "    # (1) Compute initial embedding in Poincare disk (PCA embedding)\n",
    "    X_embedded = initialization(\n",
    "        n_samples=D.shape[0], \n",
    "        n_components=2,\n",
    "        X=None,\n",
    "        random_state=seed,\n",
    "        method=\"random\",\n",
    "        init_scale=1e-4         # spread out initializations more\n",
    "    ) \n",
    "\n",
    "    # Initialize config and parameters\n",
    "    if cf == GaussianKL:\n",
    "        learning_rate = D.shape[0] / (exaggeration_factor * 10) * hyp_var\n",
    "\n",
    "    # NOTE: Change lr. depending on wrong/correct HyperbolicKL\n",
    "    elif cf == HyperbolicKL:\n",
    "        learning_rate = D.shape[0] / (exaggeration_factor * 100)\n",
    "\n",
    "    print(f\"The learning rate is: {learning_rate}\")\n",
    "\n",
    "    no_progr_its = (ex_iterations + main_iterations) / 3        # nr. of iterations of no progress before we stop\n",
    "\n",
    "    opt_conf = opt_config(cf, learning_rate, exaggeration_factor, ex_iterations, main_iterations, \n",
    "                          exact=exact_grad, vanilla=True, grad_scale_fix=grad_scale_fix, \n",
    "                          grad_fix=correct_grad, size_tol=size_tol, max_no_progress=no_progr_its)\n",
    "    opt_params = SequentialOptimizer.sequence_poincare(**opt_conf) \n",
    "\n",
    "    log_path_cf = log_path + f\"cf_{cf.class_str()}/correct_grad_{correct_gradient}/\"\n",
    "    grad_path_grad = grad_path + f\"cf_{cf.class_str()}/correct_grad_{correct_gradient}/\"\n",
    "\n",
    "    # (3) Update config params using computed variance\n",
    "    opt_params, opt_conf = initialize_logger(opt_params, opt_conf, log_path_cf, grad_path_grad)\n",
    "    opt_params[\"cf_params\"].update({\"grad_fix\" : correct_gradient})     # So the cost function knows which gradient to use\n",
    "    \n",
    "    # Only add var as param for GaussianKL\n",
    "    if cf == GaussianKL:\n",
    "        opt_params[\"cf_params\"].update({\"var\" : hyp_var})                  # GaussianKL variance for q_ij\n",
    "\n",
    "    # (4) Set up t-SNE object and run\n",
    "    htsne = HyperbolicTSNE(\n",
    "        init=X_embedded, \n",
    "        n_components=2, \n",
    "        metric=\"precomputed\",\n",
    "        verbose=1, \n",
    "        opt_method=SequentialOptimizer,         # the optimizater we use\n",
    "        opt_params=opt_params              # the parameters for the optimizers\n",
    "        )\n",
    "\n",
    "    # Compute embedding:\n",
    "    try:\n",
    "        hyperbolicEmbedding = htsne.fit_transform((D, V))\n",
    "        \n",
    "    except ValueError:\n",
    "        hyperbolicEmbedding = find_last_embedding(log_path)\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # (5) Plot the embedding (NOTE: We can also use plot_poincare)\n",
    "    # emb_fig = plot_poincare(hyperbolicEmbedding, dataLabels)\n",
    "    emb_fig = plot_tree(hyperbolicEmbedding, dataLabels)\n",
    "\n",
    "    # (6) Store experiment results\n",
    "    # folder to save results to\n",
    "    save_folder = f\"./experiment_results/experiment_{exp_id}/\"   \n",
    "\n",
    "    # dictionary containing relevant details of this experiment\n",
    "    optim_procedure = \"Vanilla SGD\"\n",
    "    description = \"Wrong gradient, tree data experiment with HyperbolicKL. More iterations, larger lr.\"\n",
    "    \n",
    "    if cf == GaussianKL:\n",
    "        exp_data = GaussianKL_Tree_results(n_children, depth, cluster_size, dist, n_nodes, htsne, ex_iterations,\n",
    "                                            main_iterations, learning_rate, cf, hyp_var, size_tol, max_dist_H, max_dist,\n",
    "                                            correct_grad, grad_scale_fix, exact_grad, exaggeration_factor, \n",
    "                                            optim_procedure, description)\n",
    "    elif cf == HyperbolicKL:\n",
    "        exp_data = HyperbolicKL_Tree_results(n_children, depth, cluster_size, dist, n_nodes, htsne, ex_iterations,\n",
    "                                             main_iterations, learning_rate, cf, correct_grad, grad_scale_fix, exact_grad, exaggeration_factor, \n",
    "                                             optim_procedure, description)\n",
    "    \n",
    "    # Prepare index for next iteration\n",
    "    exp_id += 1    \n",
    " \n",
    "    animation_step = 25\n",
    "    save_experiment_results(save_folder, None, emb_fig, opt_params, dataLabels, \n",
    "                            exp_data, hyperbolicEmbedding, animation_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delft_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
