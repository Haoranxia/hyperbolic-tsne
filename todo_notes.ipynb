{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Todo list \n",
    "Things to work on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Correct gradient term and early exaggeration relationship:**\n",
    "    - Experiment with several datasets (mnist, c_elegans custom?) with the correct gradient\n",
    "      and with/without early exaggeration to see whether there is any difference or effect?\n",
    "    - Subsample data so we can keep track of how things go about\n",
    "    - Start with custom simple dataset, controllable experiment\n",
    "    - **Experimental setup:** \n",
    "      1. Custom Hierarchical data: \n",
    "      \n",
    "         Run experiment with correct gradient for bigger and bigger datasets\n",
    "         Turn early exaggeration off, and see if clusters still form.\n",
    "         Turn early exaggeration on, and see if theres a difference.\n",
    "         Use wrong gradient and see if clusters (no early exaggeration) form.\n",
    "      2. Repeat with bigger datasets:\n",
    "         Same as above\n",
    "    \n",
    "2. **Experiment with increased attractive forces:**\n",
    "    - Take a dataset (mnist possible) and perform some experiments with differing amounts of attraction\n",
    "    - See whether that leads to embeddings where points aren't pushed to the boundary\n",
    "\n",
    "3. **Gaussian distribution for lower dimensional embedding:**\n",
    "    - Derive the gradient using a gaussian instead of a t-distribution\n",
    "    - For motivation of gaussian see teams chat\n",
    "    - Implement gaussian gradient, use on same datasets as above, compare results\n",
    "\n",
    "4. **Custom hierarchical dataset construction**\n",
    "    - Create a custom hierarchical dataset for use in all our future experiments.\n",
    "    - Clear hierarchies\n",
    "    - Trace out a \"branch\" of the tree, rotate/copy/paste it to form a '+' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperbolic space gradient analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the T-SNE paper, the t-distribution is introduced to fight against the crowding problem. So if in high dimensions we have n equidistant points, then in 2 (embedding) dimensions, we can't faithfully capture this equidistance and instead points will get squished together.\n",
    "\n",
    "\\\n",
    "\\\n",
    "**Mathematically:**\n",
    "\n",
    "$p_{ij} > 0$ get represented by $q_{ij} = 0$ because we can't faithfully model $y_i$ and $y_j$ to be close due to a lower dimensional embedding.\n",
    "\n",
    "So then when we update along the direction $(y_i - y_j)$ with (spring) factor $(p_{ij} - q_{ij})$, because $q_{ij} = 0$, we only have a attractive force due to $(p_{ij} - q_{ij}) = (p_{ij} - 0) > 0$.\n",
    "\n",
    "To make the points not crush eachother into a small area (crowding), we use a t-distribution for the lower dimensional embeddings to ensure that we can still faithfully capture the same high dimensional probabilities (using less low dimensional space), by spreading the points in low dimensions out more since we are using a heavy-tailed distribution to match affinities.\n",
    "\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "**Hyperbolic case:**\n",
    "\n",
    "In the hyperbolic scenario we might not need to use the t-distribution since space is expanded exponentially from the origin. This means that we do not have the crowding problem since a point in low dimensional space has enough space to accommodate for many equidistant points in high dimensional space due to the exponentially expanding nature of hyperbolic space.\n",
    "\n",
    "Potential experiment: Use regular gaussian instead of t-distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct gradient contribution to behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the contribution factor of the extra $d^{H}_{ij}$ in the gradient?**\n",
    "\n",
    "Enlargers the gradient of the cost function. If 2 points are far apart this factor is big. So it further accentuates repulsion or attraction depending on $(p_{ij} - q_{ij})$. Causes points to attract or repel more.\n",
    "\n",
    "Hypothesis: \n",
    "1. $d^{H}_{ij}$ probably speeds up convergence by increasing the attractive/repulsive forces initially when it matters, (when $(p_{ij} - q_{ij})$ is not approximately 0 yet)\n",
    "2. Might introduce more numerical instability, practically we might be ofshooting the edge of the unit circle. (Requires more investigation)\n",
    "\n",
    "\n",
    "\\\n",
    "\\\n",
    "**Why is it here in the hyperbolic scenario but not in the euclidean case?**\n",
    "\n",
    "Probably as an artifact from using hyperbolic space, so we need an extra factor? Investigate analytically\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Hyperbolic embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we even want hyperbolic embeddings?**\n",
    "- We think hyperbolic geometry is the underlying geometrical structure for hierarchical data\n",
    "- Naturally models tree-like (hierarchical) data due to the nature of its exponentially expanding space\n",
    "\n",
    "\n",
    "**How should we interpret visualizations on the Poincare Disk?**\n",
    "- We expect clusters inside the disk, with clusters expanding outwards from the center. \n",
    "- The expansion outwards indicates hierarchical links between clusters closer to the center, and the clusters further out\n",
    "- Clusters near the edge of the disk are very far apart from clusters in the center -> Hierarchical ordering\n",
    "- Clusters near the border are actually far apart from other eachother even if they seem close on the disk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Poincare Embeddings for Gene Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biology, and especially in the field of genetics, gene expression, it is common to want to understand the relationships between things. This is done through visualization, data analysis, etc..\n",
    "Here a brief overview of how it works, how people generally proceed, and the why's.\n",
    "\n",
    "**What is gene expression?**\n",
    "- Gene expression refers to a part of the process of how genetic material turns into observable traits\n",
    "  The most fundamental level at which the genotype (genetic material) gives rise to the phenotype (observable traits)\n",
    "\n",
    "**What is this process of gene expression?**\n",
    "- DNA:                contains your genetic material\n",
    "- Gene activation:    a step where specific genes in your DNA are activated \n",
    "- Transcription:      here the activated gene(s) get copied and stored into a molecule called mRNA\n",
    "- Translation:        the step where the mRNA molecule is used to build a protein\n",
    "- Protein:            a protein is a complex molecule that has specific uses/tasks to fulfill. This directly affects the phenotype\n",
    "\n",
    "**Why visualize gene expression in hyperbolic space?**\n",
    "- Naturally contains hierarchical structure. We can basically assume this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis notes\n",
    "\n",
    "### [1] Some notes regarding what I've run and observations\n",
    "\n",
    "**Planaria Data - 90% of data - Correct gradient - Approximate:**\\\n",
    "No instances of numerical instabilities have occurred yet as far as I know\n",
    "\n",
    "**Planaria Data - 40% of data - Correct gradient - Exact:**\\\n",
    "No numerical instability issues afaik\n",
    "\n",
    "\n",
    "### [2] Structure of the code\n",
    "Running the algorithm involves the following stages:\n",
    "1. Initialize **HyperbolicTSNE** object and run the **.fit_transform()** method\n",
    "\n",
    "2. **.fit_transform()** calls the (private) **._fit()** method \n",
    "\n",
    "3. **HyperbolicTSNE** takes in an Optimizer object. In the default case this will be the SequentialOptimizer class\n",
    "\n",
    "4. **SequentialOptimizer** requires a “config” to run properly. These are set at the beginning (before .1), through a dedicated function for initializing the config. \n",
    "\n",
    "- In the default case **.sequence_poincare()** The SequentialOptimizer works by chunking the optimization process into sequences. We can specify each chunk of the sequence through objects (a dictionary) which contains all the info for each block to run properly. These objects/blocks follow a standard format that must be adhered to We get two blocks in the SequentialOptimizer: EarlyExaggeration block and a GradientDescent block through the functions **add_block_early_exaggeration()** and **add_block_gradient_descent_with_rescale_and_gradient_mask()**\n",
    "\n",
    "- Each block must internally contain information (see the add_block functions) specifying the optimization strategy to use (e..g gradient descent), and parameters. The optimization method is specified under the “function” dictionary key,  with the “params” key containing the params for the optimization method.\n",
    "\n",
    "5. SequentialOptimizer has a **.run()** method which executes the function (optimization method). By default **.run()** calls the **.gradient_descent()** function in **solver.py**\n",
    "\n",
    "6. The optimization method is specified in the **solver.py** file. \n",
    "Most code here deals with administrative/logging tasks, and other higher level functionality regarding optimization. The default implementation is roughly split into the following parts:\n",
    "- Checks and initializing logging- Gradient computation (using cython implementation of lower level details)\n",
    "- Logging of optimization step- Convergence checks The actual computation of the optimization step is split in several parts:\n",
    "    Higher level detail in optimization method, cost function details in **cost_functions.py**, lower level details in tsne.pyx\n",
    "\n",
    "7. **cost_functions.py** contains the wrapper code that calls the lower level implementation for computing the gradient. We can find the code here for executing exact, or bh-tsne gradient descent. \n",
    "\n",
    "8. **tsne.pyx** contains the low level details of the gradient computation. Here we also find the code for the quad-tree datastructure, exponential map, and other calculation functionality.\n",
    "\n",
    "\n",
    "### [2.1] Adding new cost functions (with different parameters)\n",
    "How to add a new cost function and make it compatible with the pre-existing structure.\n",
    "\n",
    "How to use the config/params dicts to get it to do what we want: \n",
    "- Constructing the **HyperbolicTSNE()** object requires **opt_method** and **opt_params** arguments.\n",
    "- We can specify a custom **opt_method** and its associated **opt_params** \n",
    "- By default this is **SequentialOptimizer** which has **HyperbolicKL** hardcoded into it.\n",
    "  So we should modify this or come up with a new optimizer class\n",
    "- I need to explicitly update the params for the cost function to include the new embedding y_i\n",
    "- Another thing; The data is worked on flatenned. (**.ravel()**) is used during optimization. So make sure to account for that\n",
    "\n",
    "Currently the way to feed extra parameters into the CostFunction classes (to its constructor when it gets constructed) is to append a dictionary containg (k,v) pars of parameter and its value to the opt_params field in Experiment. This is a very ugly way of doing it but the current best solution ---> Refactor at some point?\n",
    "\n",
    "\n",
    "### [3] Turning on/off acceleration structure\n",
    "In the **opt_config**, we can set a boolean for the **exact** flag. We can specify exact or an approximate computation of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results analysis\n",
    "\n",
    "**Mnist dataset**\n",
    "Results for incorrect and correct gradient look the same"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delft_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
