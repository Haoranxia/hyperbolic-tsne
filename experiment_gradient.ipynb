{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please note that `empty_sequence` uses the KL divergence with Barnes-Hut approximation (angle=0.5) by default.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import traceback\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "from hyperbolicTSNE import Datasets\n",
    "from hyperbolicTSNE.visualization import plot_poincare, animate\n",
    "from hyperbolicTSNE import load_data, Datasets, SequentialOptimizer, initialization, HyperbolicTSNE\n",
    "from hyperbolicTSNE.cost_functions_ import HyperbolicKL\n",
    "from hyperbolicTSNE.util import find_last_embedding, opt_config, initialize_logger, write_data, store_visuals\n",
    "from hyperbolicTSNE.data_loaders import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_home = \"datasets\"\n",
    "log_path = \"temp/poincare/\"  # path for saving embedding snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Datasets.MNIST\n",
    "num_points = [-1]\n",
    "perplexities = [50]\n",
    "correct_gradient = False                         # NOTE: Recompile with correct flag (GRAD_FIX flag)\n",
    "exact = True                                     # Exact computation or BH estimation of gradient\n",
    "pca_components = 0                               # Whether to use pca initialization of high dim. data or not\n",
    "grad_scale_fix = True                            # Whether we multiply the gradient by the inverse metric tensor of hyperbolic space or not\n",
    "                                                 # Note that the correct hyperoblic gradient has an inverse metric tensor factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'where'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m raw_X, raw_labels \u001b[38;5;241m=\u001b[39m load_mnist(data_home\u001b[38;5;241m=\u001b[39mdata_home)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(raw_X\u001b[38;5;241m.\u001b[39mshape, raw_labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 6\u001b[0m indices_2 \u001b[38;5;241m=\u001b[39m \u001b[43mraw_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m(raw_X \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      7\u001b[0m indices_4 \u001b[38;5;241m=\u001b[39m raw_labels\u001b[38;5;241m.\u001b[39mwhere(raw_X \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(indices_2\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'where'"
     ]
    }
   ],
   "source": [
    "# Generate sample\n",
    "sample = 20         # 2 classes, for MNIST take numbers 2 and 4\n",
    "raw_X, raw_labels = load_mnist(data_home=data_home)\n",
    "print(raw_X.shape, raw_labels.shape)\n",
    "\n",
    "indices_2 = np.where(raw_X == 2)\n",
    "indices_4 = np.where(raw_X == 4)\n",
    "\n",
    "print(indices_2.shape)\n",
    "print(indices_4.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exaggeration_factor = 1\n",
    "ex_iterations = 0\n",
    "main_iterations = 100\n",
    "cf = HyperbolicKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please note that `empty_sequence` uses the KL divergence with Barnes-Hut approximation (angle=0.5) by default.\n",
      "config: {'cf': <class 'hyperbolicTSNE.cost_functions_.HyperbolicKL'>, 'learning_rate_ex': 1.0, 'learning_rate_main': 1.0, 'exaggeration': 1, 'exaggeration_its': 0, 'gradientDescent_its': 100, 'vanilla': False, 'momentum_ex': 0.5, 'momentum': 0.8, 'exact': True, 'area_split': False, 'n_iter_check': 10, 'size_tol': 0.999, 'grad_scale_fix': True}\n",
      "[HyperbolicTSNE] Received iterable as input. It should have len=2 and contain (D=None, V=None)\n",
      "[hd_mat] Warning: There is nothing to do with given parameters. Returning given D and V\n",
      "Running Gradient Descent, Verbosity: True\n",
      "[gradient_descent] Warning: because of logging, the cf will be computed at every iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gradient Descent: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Gradient Descent, Verbosity: True\n",
      "[gradient_descent] Warning: because of logging, the cf will be computed at every iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gradient Descent error: 2.03571 grad_norm: 1.22907e-02: 100%|██████████| 100/100 [00:01<00:00, 59.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# Simple experiment with no exaggeration\n",
    "\n",
    "# Compute initial embedding in Poincare disk (PCA embedding)\n",
    "X_embedded = initialization(\n",
    "    n_samples=dataX.shape[0], \n",
    "    n_components=2,\n",
    "    X=dataX,\n",
    "    random_state=seed,\n",
    "    method=\"pca\"\n",
    ") \n",
    "\n",
    "# Initialize config and parameters\n",
    "learning_rate = (dataX.shape[0] * 1) / (exaggeration_factor * 1000)\n",
    "\n",
    "opt_conf = opt_config(cf, learning_rate, exaggeration_factor, ex_iterations, main_iterations, exact)\n",
    "opt_params = SequentialOptimizer.sequence_poincare(**opt_conf) \n",
    "opt_params, opt_conf = initialize_logger(log_path, opt_params, opt_conf)\n",
    "\n",
    "# Set up H-TSNE object \n",
    "htsne = HyperbolicTSNE(\n",
    "    init=X_embedded, \n",
    "    n_components=2, \n",
    "    metric=\"precomputed\", \n",
    "    verbose=True, \n",
    "    opt_method=SequentialOptimizer,         # the optimizater we use\n",
    "    opt_params=opt_params              # the parameters for the optimizers\n",
    ")\n",
    "\n",
    "# Compute embedding:\n",
    "try:\n",
    "    hyperbolicEmbedding = htsne.fit_transform((D, V))\n",
    "    \n",
    "except ValueError:\n",
    "    hyperbolicEmbedding = find_last_embedding(log_path)\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delft_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
